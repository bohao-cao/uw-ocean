{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e30e1cc-e117-4226-91a3-9fc36e8e3d1f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: transformers[torch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu116\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.13.1+cu116 (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.13.1+cu116\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# must run on activated terminal\n",
    "# i.e source activate py310\n",
    "!pip install --user transformers[torch] datasets evaluate\n",
    "!pip install --user torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae967541-2666-4a76-b04a-4fea870329dd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting wandb\n",
      "  Downloading wandb-0.15.4-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.31)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.28.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-1.25.1-py2.py3-none-any.whl (206 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.7/206.7 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (5.4.1)\n",
      "Collecting pathtools (from wandb)\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (67.7.2)\n",
      "Collecting appdirs>=1.4.3 (from wandb)\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.19.6)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n",
      "Building wheels for collected packages: pathtools\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=391a41948e2cd4f4c631053d7d55150c7690e30583bdc544f47cfce3bbc7abee\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
      "Successfully built pathtools\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: pathtools, appdirs, setproctitle, sentry-sdk, docker-pycreds, wandb\n",
      "Successfully installed appdirs-1.4.4 docker-pycreds-0.4.0 pathtools-0.1.2 sentry-sdk-1.25.1 setproctitle-1.3.2 wandb-0.15.4\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb579470-b7ee-41b2-95b9-9088bd8a8b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_caption</th>\n",
       "      <th>username</th>\n",
       "      <th>Openness</th>\n",
       "      <th>Conscientiousness</th>\n",
       "      <th>Extroversion</th>\n",
       "      <th>Agreeableness</th>\n",
       "      <th>Neuroticism</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>caption did you know henry ford made the 8 hou...</td>\n",
       "      <td>36mktg</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>caption so clean smiling_face_with_heart-eyesr...</td>\n",
       "      <td>6staroileaters</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>caption through the night crescent_moon • maet...</td>\n",
       "      <td>740project</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>caption take your social media accounts out of...</td>\n",
       "      <td>_boostfruit.com_</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>caption appreciate you excellencealways folded...</td>\n",
       "      <td>_cloutfinder</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>caption none, is of type image, has 1 comments...</td>\n",
       "      <td>wipmarketing</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>caption enjoy this video while i disappear for...</td>\n",
       "      <td>xcarmeeen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5461</th>\n",
       "      <td>caption power back.#fanggang | xfl, is of type...</td>\n",
       "      <td>xflvipers</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>caption summeeeeer! all the hard work of clean...</td>\n",
       "      <td>yogabody_uk</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>caption now booking grammy award winning artis...</td>\n",
       "      <td>zgroupla</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>266 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           full_caption          username  \\\n",
       "59    caption did you know henry ford made the 8 hou...            36mktg   \n",
       "69    caption so clean smiling_face_with_heart-eyesr...    6staroileaters   \n",
       "26    caption through the night crescent_moon • maet...        740project   \n",
       "57    caption take your social media accounts out of...  _boostfruit.com_   \n",
       "10    caption appreciate you excellencealways folded...      _cloutfinder   \n",
       "...                                                 ...               ...   \n",
       "71    caption none, is of type image, has 1 comments...      wipmarketing   \n",
       "14    caption enjoy this video while i disappear for...         xcarmeeen   \n",
       "5461  caption power back.#fanggang | xfl, is of type...         xflvipers   \n",
       "67    caption summeeeeer! all the hard work of clean...       yogabody_uk   \n",
       "84    caption now booking grammy award winning artis...          zgroupla   \n",
       "\n",
       "      Openness  Conscientiousness  Extroversion  Agreeableness  Neuroticism  \n",
       "59           2                  2             2              2            0  \n",
       "69           0                  2             1              1            1  \n",
       "26           2                  0             0              0            1  \n",
       "57           2                  0             2              1            1  \n",
       "10           2                  0             2              1            1  \n",
       "...        ...                ...           ...            ...          ...  \n",
       "71           2                  2             2              2            0  \n",
       "14           0                  0             2              0            2  \n",
       "5461         0                  2             2              2            0  \n",
       "67           2                  2             2              2            0  \n",
       "84           2                  0             2              1            1  \n",
       "\n",
       "[266 rows x 7 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b5deed-97a6-4375-8fcf-93727618aa50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69      caption so clean smiling_face_with_heart-eyesr...\n",
       "5490    caption best day of my life 🫶heart_suit️jetzt ...\n",
       "22      caption memorial day weekenTongue_sticking_out...\n",
       "5484    caption mission accomplished. mold broken. i l...\n",
       "47      caption anyone got a pre-order on one of these...\n",
       "                              ...                        \n",
       "108     caption logo variations thinking_face.....#dig...\n",
       "24      caption wah taj!!#taj 2nd season#congratulatio...\n",
       "136     caption it’s in our dna.big love,tbx, is of ty...\n",
       "14      caption enjoy this video while i disappear for...\n",
       "5461    caption power back.#fanggang | xfl, is of type...\n",
       "Name: full_caption, Length: 66, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df[feature_df['Openness']==0]['full_caption']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c00ef2-16df-444c-8b7f-5df771e933af",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.to_parquet(\"feature.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137de9b4-d099-488a-876f-4211389a58fb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'caption best day of my life \\U0001faf6heart_suit️jetzt überall erhältlich firehundred_pointsfolded_hands•clip by: filmschieber_agency > _luck3 track by: komacasper x lucawolf_ #komacasper #vocaltekkno #vocaltekknoworldwide #lucawolf #ssb #festival #rave, is of type video, has 37 comments and 1804.0 likescaption sputnik springbreak 2023 es war absolut grandios smiling_face_with_heart-eyeshundred_pointsdankeschön für dieses unvergessliche erlebnis, ihr habt es zu dem gemacht was es war 6000 menschen die bock auf vocaltekkno hatten! jeder der dabei war darf sich persönlich von mir gedrückt fühlen heart_suit️folded_handsdankeschön bis 2024 double_exclamation_mark️smiling_face_with_halohundred_points•video by: filmschieber_agency > _luck3 #komacasper #vocaltekkno #vocaltekknoworldwide #sputnik #festival #ssb #julianluckeproductions #rave, is of type video, has 65 comments and 3522.0 likescaption der sommer ist da und hier sind meine dates für juni und juli 2023 fire\\U0001faf6wir sehen uns hundred_pointsfolded_hands#komacasper #vocaltekkno #vocaltekknoworldwide #dates #sommer #openair #festivals, is of type image, has 35 comments and 1577.0 likescaption so ihr lieben ich melde mich zu wort firefolded_handswas war da bitte beim sputnik springbreak los \\U0001f979flushed_facees hat mich so berührt das, dass zelt plötzlich so brechend voll war und wir nach 5 min einen einlass stopp hatten. face_screaming_in_fearred_heart️ ich danke euch für euren support und für diese unfassbare kranke stimmung. sign_of_the_hornssmiling_face_with_halored_heart️•clip by: filmschieber_agency > _luck3 #komacasper #vocaltekkno #vocaltekknoworldwide #sputnik #rave #bühne #julianluckeproductions, is of type video, has 168 comments and 4252.0 likescaption graf karl kassel es war fett hundred_pointsfirewir sehen uns wieder \\U0001faf6red_heart️•movie by: filmschieber_agency > jstn_mhs #komacasper #vocaltekkno #vocaltekknoworldwide #rave #club #kassel, is of type video, has 29 comments and 3208.0 likescaption am freitag den 26.05.2023 kommt meine neue single best day of my life  zusammen mit luca wolf auf allen bekannten plattformen \\U0001faf6heart_suit️fireverbreitet das ding 🥰lucawolf_ #komacasper #vocaltekkno #vocaltekknoworldwide #new #single #lucawolf, is of type video, has 151 comments and 3226.0 likescaption jaa ich freu mich aufs album fire\\U0001faf6jetzt vorbestellen (siehe bioConfusion•camera_with_flash jstn_mhs #komacasper #vocaltekkno #vocaltekknoworldwide #album, is of type image, has 17 comments and 2843.0 likescaption der schutz in die ohren und auf geht es zu 5 auftritten folded_hands\\U0001faf6ready for this smiling_face_with_sunglassesfire•camera_with_flash zandergrafie #komacasper #vocaltekkno #vocaltekknoworldwide #rave #gig, is of type image, has 21 comments and 3229.0 likescaption wilde zicke egeln was war das denn für ein saunagang 🥵 geisteskranker abriss gepaart mit 95 aufguss face_with_stuck-out_tongue_&_winking_eyeface_with_open_mouth\\u200ddashing_away\\U0001fae0bis zum nächsten mal heart_suit️firehandshake•video by: _luck3 filmschieber_agency #komacasper #vocaltekkno #vocaltekknoworldwide #wildezicke #club #rave #julianluckeproductions #_luck3, is of type video, has 82 comments and 4825.0 likescaption raven können wir alter was für ein wochenende face_screaming_in_fearfirehundred_pointskrank hundred_pointssmiling_face_with_sunglassescamera_with_flash zandergrafie #komacasper #vocaltekkno #vocaltekknoworldwide #club, is of type image, has 29 comments and 3014.0 likescaption no one else jetzt überall erhältlich hundred_pointssmiling_face_with_halolink in biodouble_exclamation_mark️#komacasper #vocaltekkno #vocaltekknoworldwide, is of type video, has 81 comments and 3188.0 likescaption vocaltekkno in fuerteventura \\U0001faf6red_heart️#komacasper #vocaltekkno #vocaltekknoworldwide #urlaub #fuerteventura #insel, is of type image, has 16 comments and 2937.0 likescaption tja jetzt steht eine 3 vor der 0 🥳face_with_tears_of_joyegal das ermutigt mich nur noch mehr abzureißen und vollgas zugeben. smiling_face_with_sunglasseshundred_pointsfiredanke an alle die zu meinem 20 hust 30 geburtstag dabei waren heart_suit️\\U0001faf6und jetzt seht selbst das fette aftermovie von _luck3 sign_of_the_hornsfolded_handssmiling_face_with_halo•movie by: filmschieber_agency _luck3 #komacasper #vocaltekkno #vocaltekknoworldwide #remix #rave #geburtstag #_luck3 #julianluckeproductions, is of type video, has 67 comments and 3906.0 likescaption double_exclamation_mark️ankündigung double_exclamation_mark️es ist soweit ich darf es euch endlich verraten face_screaming_in_fearein riesen schritt in meiner laufbahn für mich firefolded_handsbestellt das ding vor smiling_face_with_sunglassesfolded_hands\\U0001faf6 link in bio smiling_face_with_halo#komacasper #vocaltekkno #vocaltekknoworldwide #album, is of type video, has 215 comments and 5947.0 likescaption köln wir haben den affen aus dem käfig gelassen face_with_stuck-out_tongue_&_winking_eyefolded_handsfette nacht bei euch  danke smiling_face_with_sunglassesheart_suit️sign_of_the_horns•video by: _luck3 affenkaefig.events #komacasper #vocaltekkno #rave #affenkäfig #köln #_luck3 #julianluckeproductions, is of type video, has 51 comments and 4120.0 likescaption meine mai dates 2023 🤩sign_of_the_hornses wird sportlich und es geht endlich wieder nach draußen \\U0001faf6smiling_face_with_halowo sehen wir uns?•#komacasper #vocaltekkno #vocaltekknoworldwide #festival #rave #viral #openair, is of type image, has 44 comments and 1393.0 likescaption boha was ein abend in den sandsteinhöhlen halberstadt firehundred_pointsseht selber smiling_face_with_sunglassesfolded_handssmiling_face_with_halo•video by: _luck3 #komacasper #vocaltekkno #vocaltekknoworldwide #_luck3 #club #irre, is of type video, has 116 comments and 5967.0 likescaption mein buddy alfred_heinrichs und ich haben gestern mal kurz die höhle zum beben gebracht und haben zusammen mein remix von alf seiner single irre performt face_with_stuck-out_tongue_&_winking_eyekranke vibes vereint in iammelodictechno und vocaltekkno right-facing_fistleft-facing_fist\\U0001faf6smiling_face_with_halo•clip by: _luck3 #komacasper #vocaltekkno #vocaltekknoworldwide #alfredheinrichs #iammelodictechno #_luck3 #club #performance, is of type video, has 172 comments and 6140.0 likescaption emotionen und energie versprühen, dass kann ich am wochenende am besten \\U0001faf6smiling_face_with_halo#komacasper #vocaltekkno #vocaltekknoworldwide #club, is of type image, has 57 comments and 5253.0 likescaption genau mein ding smiling_face_with_sunglasses schönes bergfest fire•camera_with_flash jstn_mhs #komacasper #vocaltekkno #vocaltekknoworldwide #foto, is of type image, has 16 comments and 2733.0 likescaption hamburg meine perlesmiling_face_with_heart-eyes das war ja fett im docks hundred_points eine grandiose nacht sign_of_the_hornsfire•p.s. der track kommt am freitag den 07.04.2023 auf allen plattformen.alfred heinrichs feat. haexxa - irre (komacasper remixConfusionalfred_heinrichs supdub_records •movie by: _luck3 #komacasper #vocaltekkno #vocaltekknoworldwide #alfredheinrichs #dockshamburg #hamburg #remix #club #_luck3, is of type video, has 163 comments and 4961.0 likescaption welche energie in hamburg gestern im docks 🥵smiling_face_with_halo•camera_with_flash _luck3 #komacasper #vocaltekkno #vocaltekknoworldwide #luck3, is of type image, has 23 comments and 1906.0 likescaption meine april dates 2023sign_of_the_hornssmiling_face_with_halowo sehen und hören wir uns?see you later smiling_face_with_halosmiling_face_with_sunglasses\\U0001faf6•#dates #komacasper #vocaltekkno #vocaltekknoworldwide, is of type image, has 68 comments and 1396.0 likescaption bass,bass ich habe bass see-no-evil_monkeysmiling_face_with_sunglasses\\U0001faf6auf geht es ins wochenende hundred_points•fr: ostarena lauchhammersa: labyrinth würzburgsign_of_the_hornscamera_with_flash _luck3 #komacasper #vocaltekkno #vocaltekknoworldwide #julianluckeproductions #rave #auftritt #show #club, is of type image, has 24 comments and 2237.0 likescaption easy bang bang smiling_face_with_sunglassescentral club erfurt das war irre 🥵eine glatte 10 von 10 🤩smiling_face_with_halobis zum nächsten mal hundred_pointssign_of_the_horns•video by: _luck3 #komacasper #vocaltekkno #vocaltekknoworldwide #julianluckeproductions #_luck3 #erfurt #club #rave, is of type video, has 54 comments and 4093.0 likescaption das wochenende bleibt in erinnerung!🥵3 dicke shows und dann der krönende abschluss in erfurt im central club smiling_face_with_halofiredanke für diese energie red_heart️\\U0001faf6•camera_with_flash by _luck3 #komacasper #vocaltekkno #vocaltekknoworldwide #club #show #erfurt #wochenende, is of type image, has 12 comments and 1788.0 likescaption dieses jahr wird es vocaltekkno baby face_with_stuck-out_tongue_&_winking_eyehundred_pointsfirecamera_with_flash jstn_mhs #komacasper #vocaltekkno #vocaltekknoworldwide, is of type image, has 35 comments and 3879.0 likescaption diese energie und kreativität die durch meine adern fließt ist enorm 🤩 dieses jahr werdet ihr davon noch viel zu spüren bekommen \\U0001fae1 versprochensmiling_face_with_smiling_eyes•camera_with_flash marvin_schoenfelder #komacasper #vocaltekkno #vocaltekknoworldwide #club #energie #rave #viral, is of type image, has 15 comments and 2218.0 likescaption seit langem mal wieder im e werk reichenbach, was eine stimmung und ein genialer abriss \\U0001faf6\\U0001fae1bis zum nächsten mal fireperson_raising_hand\\u200dmale_sign️🥰•movie by: _luck3 #komacasper #vocaltekkno #vocaltekknoworldwide #_luck3 #julianluckeproductions #club #remix, is of type video, has 65 comments and 3967.0 likescaption auf geht es ins wochenende grinning_face_with_smiling_eyescamera_with_flash _luck3 #komacasper #vocaltekkno #vocaltekknoworldwide #julianluckeproductions #rave, is of type image, has 13 comments and 2603.0 likescaption dates märz 2023 hundred_pointssign_of_the_horns#komacasper #vocaltekkno #vocaltekknoworldwide #dates, is of type image, has 15 comments and 1068.0 likescaption kick that bass 🤩firehundred_pointscamera_with_flash marvin_schoenfelder #komacasper #vocaltekkno #vocaltekknoworldwide #club, is of type image, has 9 comments and 2116.0 likescaption neues lied, geiler club, geile crowd 🥵sweat_dropletsich hatte die ehre,,better days‘‘ in zusammenarbeit mit blvckcrowz in der factory magdeburg zu spielen und die stimmung war einfach krank, aber seht selber \\U0001faf6fireihr könnt den track jetzt überall streamen smiling_face_with_sunglassesdanke an craigmortalis für die einladung handshakered_heart️•riesen respekt an _luck3 für diese krassen momentaufnahme einer vollkommensten eskalation smiling_face_with_heart-eyes\\U0001fae1\\U0001faf6red_heart️#komacasper #blvckcrowz #vocaltekkno #vocaltekknoworldwide #club #single #new, is of type video, has 33 comments and 2440.0 likescaption alwo altenburg war wild, habe gehört wir sehen uns im april wieder sign_of_the_hornshundred_pointsfire•movie by: _luck3 #komacasper #vocaltekkno #vocaltekknoworldwide #alwoaltenburg #julianluckeproductions, is of type video, has 17 comments and 2541.0 likescaption rave & bass & vocaltekkno smiling_face_with_sunglassessign_of_the_hornsfire•camera_with_flash _luck3 #komacasper #vocaltekkno #vocaltekknoworldwide #julianluckeproductions #club #rave, is of type image, has 9 comments and 2040.0 likescaption heute 00:00 uhr sign_of_the_horns🥰hundred_pointspresave link in story sign_of_the_hornsfire#komacasper #vocaltekkno #vocaltekknoworldwide #blvckcrowz #collab, is of type video, has 2 comments and 43.0 likescaption mauerpfeiffer saarbrücken am freitag war einfach ne fette party smiling_face_with_heart-eyessign_of_the_hornshundred_pointstrack kommt am 24.02.2023 auf allen plattformen smiling_face_with_heart-eyessmiling_face_with_halohundred_pointsmovie by: _luck3 #komacasper #vocaltekkno #vocaltekknoworldwide #saarbrücken #release #club #julianluckeproductions, is of type video, has 49 comments and 3197.0 likescaption diesen donnerstag hundred_pointsfireihr könnt das teil jetzt presaven \\U0001faf6\\U0001fae1(siehe storyConfusion#komacasper #blvckcrowz #collab, is of type video, has 18 comments and 363.0 likescaption was für ein grandioses wochenende 4 auftritte 1600 km, viel liebe und eine absolute eskalation 🥵\\U0001faf6das herz ist für jeden der dabei war und es mit mir/uns genossen hat \\U0001faf6red_heart️dankemauerpfeiffer ilovesnow.festival alwoclub strezzkidz_official stellwerk_halle •einen schönen sonntag euch, auf das nächste wochenende \\U0001fae1firecamera_with_flash _luck3 #komacasper #vocaltekkno #vocaltekknoworldwide #club #deutschland #julianluckeproductions #ifeelthebass, is of type image, has 33 comments and 3286.0 likescaption mauerpfeiffer saarbrücken war so dick, immer noch sprachlos über die gestrige nacht red_heart️fire🥵dankeschön smiling_face_with_halored_heart️clip by: _luck3 #komacasper #vocaltekkno #vocaltekknoworldwide #saarbrücken #mauerpfeiffer #club #crowd #julianluckeproductions, is of type video, has 35 comments and 3231.0 likescaption i feel the bass fire🥵•camera_with_flash _luck3 #komacasper #vocaltekkno #vocaltekknoworldwide #julianluckeproductions, is of type image, has 20 comments and 3013.0 likescaption helios köln es war mir eine ehre smiling_face_with_sunglasseshandshakesmiling_face_with_halofireich komme wieder das ist safe sign_of_the_horns🤪•movie by: _luck3 #komacasper #vocaltekkno #vocaltekknoworldwide #köln #club #luck_3 #julianluckeproductions #unreal, is of type video, has 30 comments and 3027.0 likescaption dankeschön helios köln sign_of_the_hornshundred_pointsdas war stabil und ich glaube wir haben die gebruederbrett_insta sportlich vertreten \\U0001fae1red_heart️camera_with_flash _luck3 #komacasper #vocaltekkno #vocaltekknoworldwide #julianluckeproductions #köln #club, is of type image, has 6 comments and 1566.0 likescaption meine februar dates 2023 \\U0001faf6fireich freu mich auf ein dicken monat face_with_stuck-out_tongue_&_closed_eyeshundred_points#komacasper #vocaltekkno #vocaltekknoworldwide #dates #february, is of type image, has 51 comments and 1254.0 likescaption ey was war da los im sax dölzig 🥵 1300 leute am eskalieren face_with_open_mouth_&_cold_sweatfett sign_of_the_hornsfiredanke für diese fette nacht smiling_face_with_halo🥰\\U0001faf6•video by: marvin_schoenfelder #komacasper #vocaltekkno #vocaltekknoworldwide #club #remix #dölzig, is of type video, has 54 comments and 3957.0 likescaption wow was für ein wochenende smiling_face_with_sunglassesfirehirsch nürnberg und sax dölzig danke für diesen stabilen tanz \\U0001fae1red_heart️•camera_with_flash marvin_schoenfelder #komacasper #vocaltekkno #vocaltekknoworldwide #club #marvinschoenfelder, is of type image, has 11 comments and 2170.0 likescaption ich hänge immer noch im letzten wochenende fest 🤭smiling_face_with_sunglasseswien war stark und ich bin sprachlos.🇦🇹handshakedie tage im studio sind auch ein genuss der kreativität smiling_face_with_halo\\U0001faf6bald mehr dazu, schönes wochenende euch allen 🥰camera_with_flash mikejonesfotografie #komacasper #vocaltekkno #wien #glücklich #affenkäfig, is of type image, has 16 comments and 1761.0 likescaption alter österreich 🇦🇹 das war geisteskrank smiling_face_with_heart-eyesfiredanke das ihr mich so lieb empfangen habt in einem neuen land, es war eine wahnsinnig tolle erfahrung in einem anderen land zu spielen,dafür bin ich sehr dankbar!red_heart️dankeschön wien und dankeschön an die affenkäfig crewred_heart️p.s. der track kommt am 09.02.2023 es ist eine fette collab zusammen mit meinem bruder blvck crowz right-facing_fistleft-facing_fist•movie by: _luck3 track by: blvckcrowz _komacasper_ #komacasper #vocaltekkno #vocaltekknoworldwide #newmusic #viral #wien #ostereich #affenkäfig #julianluckeproductions, is of type video, has 67 comments and 3262.0 likescaption du,du,du und du backhand_index_pointing_downthanks for the support hundred_pointssmiling_face_with_halored_heart️#komacasper #vocaltekkno #vocaltekknoworldwide, is of type image, has 16 comments and 2117.0 likescaption 2023 ich bin readyfiresmiling_face_with_haloneue musik,neue festivals,neue clubs.ich bin hyped auf ein ein aufregendes jahr smiling_face_with_sunglassesfire#komacasper #vocaltekkno #vocaltekknoworldwide #2023 #club #festivals, is of type image, has 8 comments and 1819.0 likes'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth=100\n",
    "feature_df[feature_df['Openness']==0].full_caption.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f451131b-6f15-4482-a569-77e390dd61b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0    200\n",
       "0.0     66\n",
       "Name: Openness, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df['Openness'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b2c2af-dd17-4c13-9153-e32e947eef0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    61\n",
       "2.0     1\n",
       "Name: Conscientiousness, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df['Conscientiousness'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900b5c6b-848d-455d-9392-16fc202c058d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    229\n",
       "0     27\n",
       "1     10\n",
       "Name: Extroversion, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df['Extroversion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052af9c9-373b-411d-99cb-f418bf2f3de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    139\n",
       "0     82\n",
       "1     45\n",
       "Name: Agreeableness, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df['Agreeableness'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c077b0f-320b-4416-81e5-0c5b7ff7ec85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    194\n",
       "1     45\n",
       "2     27\n",
       "Name: Neuroticism, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df['Neuroticism'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26a7446d-19cf-4041-b742-c3ef6d5050f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/266 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13472 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4864 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3007 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25039 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/266 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 8977\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2245\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from transformers import AutoTokenizer\n",
    "# using disilroberta as a quick baseline model\n",
    "# https://huggingface.co/docs/transformers/tasks/sequence_classification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\", use_fast=True)\n",
    "\n",
    "focused_trait = \"Neuroticism\"\n",
    "\n",
    "\n",
    "def tokenize_input(examples):\n",
    "    return tokenizer(examples[\"full_caption\"])\n",
    "\n",
    "def group_texts(examples):\n",
    "    block_size = 128    \n",
    "    \n",
    "    result= defaultdict(list)\n",
    "    for i, input_id  in enumerate(examples['input_ids']):\n",
    "        attn_mask = examples['attention_mask'][i]\n",
    "        total_length = len(input_id)\n",
    "        if total_length > block_size:\n",
    "            total_length = (total_length // block_size) * block_size\n",
    "        num_chunks = total_length // block_size\n",
    "        result['input_ids'].extend([input_id[i : i + block_size] for i in range(0, total_length, block_size)])\n",
    "        result['attention_mask'].extend([attn_mask[i : i + block_size] for i in range(0, total_length, block_size)])\n",
    "        result['Openness'].extend([examples['Openness'][i] for _ in range(num_chunks)])\n",
    "        result['Conscientiousness'].extend([examples['Conscientiousness'][i] for _ in range(num_chunks)])\n",
    "        result['Extroversion'].extend([examples['Extroversion'][i] for _ in range(num_chunks)])\n",
    "        result['Agreeableness'].extend([examples['Agreeableness'][i] for _ in range(num_chunks)])\n",
    "        result['Neuroticism'].extend([examples['Neuroticism'][i] for _ in range(num_chunks)])    \n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "import pandas as pd \n",
    "all_five = ['Openness', 'Conscientiousness','Extroversion', 'Agreeableness', 'Neuroticism']\n",
    "feature_df = pd.read_parquet(\"feature.parquet\")\n",
    "feature_df = feature_df.dropna()\n",
    "\n",
    "d = {0.5:1,1.0:2,0:0}\n",
    "for column in all_five:\n",
    "    feature_df[column] = feature_df[column].map(d)\n",
    "    feature_df.astype({column:'int32'}).dtypes\n",
    "\n",
    "all_five.remove(focused_trait)\n",
    "\n",
    "ds = Dataset.from_dict(feature_df.to_dict('list'))\n",
    "tokenized_ds = ds.map(tokenize_input, batched=True, num_proc=4, remove_columns=['full_caption','username'])\n",
    "lm_dataset_ig = tokenized_ds.map(group_texts, num_proc=4, batched=True)\n",
    "lm_dataset_ig = lm_dataset_ig.shuffle(seed=42)\n",
    "lm_dataset_ig = lm_dataset_ig.rename_column(focused_trait, \"label\")\n",
    "lm_dataset_ig = lm_dataset_ig.remove_columns(all_five)\n",
    "lm_dataset_ig = lm_dataset_ig.train_test_split(test_size=0.2)\n",
    "lm_dataset_ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6939a417-9b6c-4e23-8066-1ac20e42b2a4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_dataset_ig['train']['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba6d5c4-ee1d-4467-b3f5-19aef89a8911",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dataset IO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef70b124-bc97-4e03-8346-3befd08a96a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/8977 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2245 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_dataset_ig.save_to_disk(\"distilbert_base_block128_train_test_split_tokenized_feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249a27ce-5775-4ca4-865e-ae93ba7f5913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "lm_dataset_ig = load_from_disk(\"distilbert_base_block128_train_test_split_tokenized_feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9638013-98af-44c3-94fc-8f2b3fcb7ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename Agreeableness to label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545caef6-9b6a-48de-bba4-9b259c7eadca",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " ...]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_dataset_ig['train']['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edb6dac6-a840-4fa4-8950-ff2a98033dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    print(f\"predictions:{predictions}, item_freq:{np.unique(predictions, return_counts=True)}.\")\n",
    "    print(f\"labels:{labels}, item_freq:{np.unique(labels, return_counts=True)}.\")\n",
    "    accuracy_measurement =  accuracy.compute(predictions=predictions, references=labels)\n",
    "    precision_measurement =  precision.compute(predictions=predictions, references=labels, average='micro')\n",
    "    recall_measurement =  recall.compute(predictions=predictions, references=labels, average='micro')\n",
    "    print(f\"Accuracy:{accuracy_measurement}, Precision:{precision_measurement}, Recall:{recall_measurement}\")\n",
    "    return precision_measurement\n",
    "\n",
    "id2label = {0: \"LOW\", 1: \"VAGUE\", 2: \"HIGH\"}\n",
    "label2id = {\"LOW\": 0, \"VAGUE\":1, \"HIGH\": 2}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=3, id2label=id2label, label2id=label2id\n",
    ")\n",
    "import os \n",
    "os.environ[\"WANDB_PROJECT\"]=\"upwork-ocean-classification\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"]=\"true\"\n",
    "os.environ[\"WANDB_WATCH\"]=\"false\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"distilbert-base-uncased-{focused_trait}-classification-shuffled\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"wandb\",\n",
    "    push_to_hub=False,\n",
    "    run_name=focused_trait\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset_ig[\"train\"],\n",
    "    eval_dataset=lm_dataset_ig[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78b49851-07e0-4d84-bb23-d8a9be446c21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/uw-ocean/wandb/run-20230620_001307-tj18nhki</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bohao-cao/upwork-ocean-classification/runs/tj18nhki' target=\"_blank\">Neuroticism</a></strong> to <a href='https://wandb.ai/bohao-cao/upwork-ocean-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bohao-cao/upwork-ocean-classification' target=\"_blank\">https://wandb.ai/bohao-cao/upwork-ocean-classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bohao-cao/upwork-ocean-classification/runs/tj18nhki' target=\"_blank\">https://wandb.ai/bohao-cao/upwork-ocean-classification/runs/tj18nhki</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2810' max='2810' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2810/2810 09:40, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.522900</td>\n",
       "      <td>0.325799</td>\n",
       "      <td>0.885969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.258100</td>\n",
       "      <td>0.304947</td>\n",
       "      <td>0.909577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.152200</td>\n",
       "      <td>0.251792</td>\n",
       "      <td>0.938085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.083800</td>\n",
       "      <td>0.260075</td>\n",
       "      <td>0.946548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.042900</td>\n",
       "      <td>0.254726</td>\n",
       "      <td>0.946993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions:[0 0 0 ... 1 0 0], item_freq:(array([0, 1, 2]), array([1845,  360,   40])).\n",
      "labels:[0 0 0 ... 1 0 0], item_freq:(array([0, 1, 2]), array([1733,  356,  156])).\n",
      "Accuracy:{'accuracy': 0.8859688195991091}, Precision:{'precision': 0.8859688195991091}, Recall:{'recall': 0.8859688195991091}\n",
      "predictions:[0 0 0 ... 1 0 0], item_freq:(array([0, 1, 2]), array([1816,  375,   54])).\n",
      "labels:[0 0 0 ... 1 0 0], item_freq:(array([0, 1, 2]), array([1733,  356,  156])).\n",
      "Accuracy:{'accuracy': 0.9095768374164811}, Precision:{'precision': 0.9095768374164811}, Recall:{'recall': 0.9095768374164811}\n",
      "predictions:[0 0 0 ... 1 0 0], item_freq:(array([0, 1, 2]), array([1826,  283,  136])).\n",
      "labels:[0 0 0 ... 1 0 0], item_freq:(array([0, 1, 2]), array([1733,  356,  156])).\n",
      "Accuracy:{'accuracy': 0.9380846325167038}, Precision:{'precision': 0.9380846325167038}, Recall:{'recall': 0.9380846325167038}\n",
      "predictions:[0 0 0 ... 1 0 0], item_freq:(array([0, 1, 2]), array([1779,  317,  149])).\n",
      "labels:[0 0 0 ... 1 0 0], item_freq:(array([0, 1, 2]), array([1733,  356,  156])).\n",
      "Accuracy:{'accuracy': 0.9465478841870824}, Precision:{'precision': 0.9465478841870824}, Recall:{'recall': 0.9465478841870824}\n",
      "predictions:[0 0 0 ... 1 0 0], item_freq:(array([0, 1, 2]), array([1778,  328,  139])).\n",
      "labels:[0 0 0 ... 1 0 0], item_freq:(array([0, 1, 2]), array([1733,  356,  156])).\n",
      "Accuracy:{'accuracy': 0.9469933184855234}, Precision:{'precision': 0.9469933184855234}, Recall:{'recall': 0.9469933184855234}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▆▁▂▁</td></tr><tr><td>eval/precision</td><td>▁▄▇██</td></tr><tr><td>eval/runtime</td><td>▅█▄▂▁</td></tr><tr><td>eval/samples_per_second</td><td>▄▁▅▇█</td></tr><tr><td>eval/steps_per_second</td><td>▄▁▅▇█</td></tr><tr><td>train/epoch</td><td>▁▁▃▃▄▅▆▆▇██</td></tr><tr><td>train/global_step</td><td>▁▁▃▃▄▅▆▆▇██</td></tr><tr><td>train/learning_rate</td><td>█▆▄▃▁</td></tr><tr><td>train/loss</td><td>█▄▃▂▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.25473</td></tr><tr><td>eval/precision</td><td>0.94699</td></tr><tr><td>eval/runtime</td><td>8.6008</td></tr><tr><td>eval/samples_per_second</td><td>261.024</td></tr><tr><td>eval/steps_per_second</td><td>16.394</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>2810</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0429</td></tr><tr><td>train/total_flos</td><td>1486476305890560.0</td></tr><tr><td>train/train_loss</td><td>0.19167</td></tr><tr><td>train/train_runtime</td><td>585.4364</td></tr><tr><td>train/train_samples_per_second</td><td>76.669</td></tr><tr><td>train/train_steps_per_second</td><td>4.8</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Neuroticism</strong> at: <a href='https://wandb.ai/bohao-cao/upwork-ocean-classification/runs/tj18nhki' target=\"_blank\">https://wandb.ai/bohao-cao/upwork-ocean-classification/runs/tj18nhki</a><br/>Synced 6 W&B file(s), 0 media file(s), 7 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230620_001307-tj18nhki/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "trainer.train()\n",
    "wandb.finish()\n",
    "trainer.save_state()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fa1111-b862-4623-ba37-4bc37c962163",
   "metadata": {},
   "source": [
    "## inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9f1a82e1-03de-4a54-8c7e-d39775bb69a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "inference_dataset = pd.read_parquet(\"prediction_feature.parquet\")\n",
    "prediction_dataset = pd.read_parquet(\"concat_feature.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a92228a1-b15b-4f2b-a507-3425f7a2ae06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_caption</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ig_username</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>007offmason</th>\n",
       "      <td>caption out now on all platformsglobe_showing_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0g.ed1tz</th>\n",
       "      <td>caption none, is of type image, has 6 comments...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000taraneh</th>\n",
       "      <td>caption .«لج و لج بازی» با صدای زنده یاد پوران...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000thingstodoindenver</th>\n",
       "      <td>caption opening day specials: here are some fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101_molly</th>\n",
       "      <td>caption yessirr let’s go!! firefire🩸red_heart️...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zuber_zooby</th>\n",
       "      <td>caption titanic movie explained in hindi face_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zuraidajardine</th>\n",
       "      <td>caption unhappy fathers day 🩶today can hit dif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zuri_baby</th>\n",
       "      <td>caption never in a million years did i think i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zyramusicofficial</th>\n",
       "      <td>caption thank you leannealamira for working yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zzward</th>\n",
       "      <td>caption meet my stylist!!! haha! #independenta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14240 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             full_caption\n",
       "ig_username                                                              \n",
       "007offmason             caption out now on all platformsglobe_showing_...\n",
       "0g.ed1tz                caption none, is of type image, has 6 comments...\n",
       "1000taraneh             caption .«لج و لج بازی» با صدای زنده یاد پوران...\n",
       "1000thingstodoindenver  caption opening day specials: here are some fu...\n",
       "101_molly               caption yessirr let’s go!! firefire🩸red_heart️...\n",
       "...                                                                   ...\n",
       "zuber_zooby             caption titanic movie explained in hindi face_...\n",
       "zuraidajardine          caption unhappy fathers day 🩶today can hit dif...\n",
       "zuri_baby               caption never in a million years did i think i...\n",
       "zyramusicofficial       caption thank you leannealamira for working yo...\n",
       "zzward                  caption meet my stylist!!! haha! #independenta...\n",
       "\n",
       "[14240 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "08443d39-88e4-4a9e-8b29-146ba29d1ebf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/14240 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from collections import defaultdict\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\", use_fast=True)\n",
    "\n",
    "def tokenize_input(examples):\n",
    "    return tokenizer(examples[\"full_caption\"], padding=True)\n",
    "\n",
    "def group_texts_for_inference(examples):\n",
    "    block_size = 512    \n",
    "    \n",
    "    result= defaultdict(list)\n",
    "    for i, input_id  in enumerate(examples['input_ids']):\n",
    "        attn_mask = examples['attention_mask'][i]\n",
    "        total_length = len(input_id)\n",
    "        if total_length < block_size:\n",
    "            result['input_ids'].extend([input_id])\n",
    "            result['attention_mask'].extend([attn_mask])\n",
    "            result['ig_username'].extend([examples['ig_username'][i]])\n",
    "            continue                        \n",
    "        if total_length > block_size:\n",
    "            total_length = (total_length // block_size) * block_size\n",
    "        num_chunks = total_length // block_size\n",
    "        with open (\"log.log\", \"a\") as f_out:\n",
    "            f_out.write(f\"{i} {total_length} {block_size} {num_chunks}\\n\")\n",
    "        result['input_ids'].extend([input_id[i : i + block_size] for i in range(0, total_length, block_size)])\n",
    "        result['attention_mask'].extend([attn_mask[i : i + block_size] for i in range(0, total_length, block_size)])\n",
    "        result['ig_username'].extend([examples['ig_username'][i] for _ in range(num_chunks)])\n",
    "    return result\n",
    "\n",
    "\n",
    "#ds = Dataset.from_dict(inference_dataset.reset_index().to_dict('list'))\n",
    "#tokenized_ds = ds.map(tokenize_input, batched=True, num_proc=4, remove_columns=['full_caption'])\n",
    "lm_dataset_ig = tokenized_ds.map(group_texts_for_inference, num_proc=4, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25368aa9-cf92-4d0f-937f-04cdba797060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lm_dataset_ig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2475e162-bd30-42eb-b739-3cee07de97b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "lm_dataset_ig = load_from_disk(\"grouped_prediction_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1998c77c-1ff9-49a8-afd6-374d59c3b739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/3 shards):   0%|          | 0/409880 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_dataset_ig.save_to_disk(\"grouped_prediction_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6ecf1e9-321f-4115-b5b2-100992cf1084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/14240 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_ds.save_to_disk(\"tokenized_prediction_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c92109f-312b-4c7c-958d-fea517127e8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DistilBertModel, DistilBertForSequenceClassification\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lm_dataset_ig_gpu = lm_dataset_ig.with_format(\"torch\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "761e125d-5975-47db-970a-acaaedc4a5d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-01 02:08:50.618016: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-01 02:08:53.296281: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-07-01 02:08:53.296427: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-07-01 02:08:53.296439: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "focused_trait = \"Agreeableness\"\n",
    "ft_model = AutoModelForSequenceClassification.from_pretrained(f\"distilbert-base-uncased-{focused_trait}-classification-shuffled\")\n",
    "ft_model = ft_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6475fc-df58-49a1-bab9-251b1884be77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "40\n",
      "80\n",
      "120\n",
      "160\n",
      "200\n",
      "240\n",
      "280\n",
      "320\n",
      "360\n",
      "400\n",
      "440\n",
      "480\n",
      "520\n",
      "560\n",
      "600\n",
      "640\n",
      "680\n",
      "720\n",
      "760\n",
      "800\n",
      "840\n",
      "880\n",
      "920\n",
      "960\n",
      "1000\n",
      "1040\n",
      "1080\n",
      "1120\n",
      "1160\n",
      "1200\n",
      "1240\n",
      "1280\n",
      "1320\n",
      "1360\n",
      "1400\n",
      "1440\n",
      "1480\n",
      "1520\n",
      "1560\n",
      "1600\n",
      "1640\n",
      "1680\n",
      "1720\n",
      "1760\n",
      "1800\n",
      "1840\n",
      "1880\n",
      "1920\n",
      "1960\n",
      "2000\n",
      "2040\n",
      "2080\n",
      "2120\n",
      "2160\n",
      "2200\n",
      "2240\n",
      "2280\n",
      "2320\n",
      "2360\n",
      "2400\n",
      "2440\n",
      "2480\n",
      "2520\n",
      "2560\n",
      "2600\n",
      "2640\n",
      "2680\n",
      "2720\n",
      "2760\n",
      "2800\n",
      "2840\n",
      "2880\n",
      "2920\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "all_predictions = pd.DataFrame()\n",
    "batch_size = 40\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(lm_dataset_ig_gpu), batch_size):\n",
    "        predictions = ft_model(\n",
    "            input_ids=lm_dataset_ig_gpu['input_ids'][i: i+batch_size],\n",
    "            attention_mask=lm_dataset_ig_gpu['attention_mask'][i: i+batch_size]\n",
    "        ).logits.argmax(axis=1)\n",
    "        predictions_df_current_batch = pd.DataFrame(\n",
    "            {\n",
    "                \"usernames\":lm_dataset_ig['ig_username'][i: i+batch_size], \n",
    "                \"predictions\":predictions.to('cpu')}\n",
    "            )\n",
    "        pd.concat([all_predictions, predictions_df_current_batch])\n",
    "        if i%10==0:\n",
    "            print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c1bb2b-9749-4c29-a448-9477dd96e75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Post process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "687fc22f-8cc8-4598-b5d4-3abe538817e2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">predictions</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>list</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>usernames</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2jpantoja</th>\n",
       "      <td>[2, 0]</td>\n",
       "      <td>{0, 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_moneybo</th>\n",
       "      <td>[2, 2, 2]</td>\n",
       "      <td>{2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_ts97__</th>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2]</td>\n",
       "      <td>{2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aless_capparelli</th>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>{0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alexsaenz</th>\n",
       "      <td>[2, 1, 1, 1]</td>\n",
       "      <td>{1, 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vngnc</th>\n",
       "      <td>[2, 2, 2, 1, 1, 0]</td>\n",
       "      <td>{0, 1, 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>waynebutlercomedy</th>\n",
       "      <td>[2, 2, 2, 2]</td>\n",
       "      <td>{2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>woolimusic</th>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>{0, 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zairmontes</th>\n",
       "      <td>[2, 1, 0, 2, 2, 2, 2, 2]</td>\n",
       "      <td>{0, 1, 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zambezijuice</th>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>{0}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   predictions           \n",
       "                                          list        set\n",
       "usernames                                                \n",
       "2jpantoja                               [2, 0]     {0, 2}\n",
       "_moneybo                             [2, 2, 2]        {2}\n",
       "_ts97__            [2, 2, 2, 2, 2, 2, 2, 2, 2]        {2}\n",
       "aless_capparelli               [0, 0, 0, 0, 0]        {0}\n",
       "alexsaenz                         [2, 1, 1, 1]     {1, 2}\n",
       "...                                        ...        ...\n",
       "vngnc                       [2, 2, 2, 1, 1, 0]  {0, 1, 2}\n",
       "waynebutlercomedy                 [2, 2, 2, 2]        {2}\n",
       "woolimusic                              [1, 0]     {0, 1}\n",
       "zairmontes            [2, 1, 0, 2, 2, 2, 2, 2]  {0, 1, 2}\n",
       "zambezijuice                   [0, 0, 0, 0, 0]        {0}\n",
       "\n",
       "[88 rows x 2 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df = predictions_df.groupby('usernames').agg([list, set])\n",
    "predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ff70ded9-2cb3-478d-9b65-3f64bd623cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df.columns = predictions_df.columns.to_flat_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "813a6255-fbf3-46f6-a752-394bc41c279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = predictions_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e6fa63f1-a2bf-4346-8191-416e93fc03e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>usernames</th>\n",
       "      <th>(predictions, list)</th>\n",
       "      <th>(predictions, set)</th>\n",
       "      <th>predictions_count</th>\n",
       "      <th>predictions_unique_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2jpantoja</td>\n",
       "      <td>[2, 0]</td>\n",
       "      <td>{0, 2}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>_moneybo</td>\n",
       "      <td>[2, 2, 2]</td>\n",
       "      <td>{2}</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_ts97__</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2]</td>\n",
       "      <td>{2}</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aless_capparelli</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>{0}</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alexsaenz</td>\n",
       "      <td>[2, 1, 1, 1]</td>\n",
       "      <td>{1, 2}</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>vngnc</td>\n",
       "      <td>[2, 2, 2, 1, 1, 0]</td>\n",
       "      <td>{0, 1, 2}</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>waynebutlercomedy</td>\n",
       "      <td>[2, 2, 2, 2]</td>\n",
       "      <td>{2}</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>woolimusic</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>{0, 1}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>zairmontes</td>\n",
       "      <td>[2, 1, 0, 2, 2, 2, 2, 2]</td>\n",
       "      <td>{0, 1, 2}</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>zambezijuice</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>{0}</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            usernames          (predictions, list) (predictions, set)  \\\n",
       "0           2jpantoja                       [2, 0]             {0, 2}   \n",
       "1            _moneybo                    [2, 2, 2]                {2}   \n",
       "2             _ts97__  [2, 2, 2, 2, 2, 2, 2, 2, 2]                {2}   \n",
       "3    aless_capparelli              [0, 0, 0, 0, 0]                {0}   \n",
       "4           alexsaenz                 [2, 1, 1, 1]             {1, 2}   \n",
       "..                ...                          ...                ...   \n",
       "83              vngnc           [2, 2, 2, 1, 1, 0]          {0, 1, 2}   \n",
       "84  waynebutlercomedy                 [2, 2, 2, 2]                {2}   \n",
       "85         woolimusic                       [1, 0]             {0, 1}   \n",
       "86         zairmontes     [2, 1, 0, 2, 2, 2, 2, 2]          {0, 1, 2}   \n",
       "87       zambezijuice              [0, 0, 0, 0, 0]                {0}   \n",
       "\n",
       "    predictions_count  predictions_unique_count  \n",
       "0                   2                         2  \n",
       "1                   3                         1  \n",
       "2                   9                         1  \n",
       "3                   5                         1  \n",
       "4                   4                         2  \n",
       "..                ...                       ...  \n",
       "83                  6                         3  \n",
       "84                  4                         1  \n",
       "85                  2                         2  \n",
       "86                  8                         3  \n",
       "87                  5                         1  \n",
       "\n",
       "[88 rows x 5 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df['predictions_count'] = predictions_df.apply(lambda r: len(r[('predictions','list')]), axis=1)\n",
    "predictions_df['predictions_unique_count'] = predictions_df.apply(lambda r: len(r[('predictions','set')]), axis=1)\n",
    "predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f9841740-eb4d-4a1d-9164-f740b02df10e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>usernames</th>\n",
       "      <th>(predictions, list)</th>\n",
       "      <th>(predictions, set)</th>\n",
       "      <th>predictions_count</th>\n",
       "      <th>predictions_unique_count</th>\n",
       "      <th>predictions_final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2jpantoja</td>\n",
       "      <td>[2, 0]</td>\n",
       "      <td>{0, 2}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>_moneybo</td>\n",
       "      <td>[2, 2, 2]</td>\n",
       "      <td>{2}</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_ts97__</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2]</td>\n",
       "      <td>{2}</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aless_capparelli</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>{0}</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alexsaenz</td>\n",
       "      <td>[2, 1, 1, 1]</td>\n",
       "      <td>{1, 2}</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>vngnc</td>\n",
       "      <td>[2, 2, 2, 1, 1, 0]</td>\n",
       "      <td>{0, 1, 2}</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>waynebutlercomedy</td>\n",
       "      <td>[2, 2, 2, 2]</td>\n",
       "      <td>{2}</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>woolimusic</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>{0, 1}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>zairmontes</td>\n",
       "      <td>[2, 1, 0, 2, 2, 2, 2, 2]</td>\n",
       "      <td>{0, 1, 2}</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>zambezijuice</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>{0}</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            usernames          (predictions, list) (predictions, set)  \\\n",
       "0           2jpantoja                       [2, 0]             {0, 2}   \n",
       "1            _moneybo                    [2, 2, 2]                {2}   \n",
       "2             _ts97__  [2, 2, 2, 2, 2, 2, 2, 2, 2]                {2}   \n",
       "3    aless_capparelli              [0, 0, 0, 0, 0]                {0}   \n",
       "4           alexsaenz                 [2, 1, 1, 1]             {1, 2}   \n",
       "..                ...                          ...                ...   \n",
       "83              vngnc           [2, 2, 2, 1, 1, 0]          {0, 1, 2}   \n",
       "84  waynebutlercomedy                 [2, 2, 2, 2]                {2}   \n",
       "85         woolimusic                       [1, 0]             {0, 1}   \n",
       "86         zairmontes     [2, 1, 0, 2, 2, 2, 2, 2]          {0, 1, 2}   \n",
       "87       zambezijuice              [0, 0, 0, 0, 0]                {0}   \n",
       "\n",
       "    predictions_count  predictions_unique_count  predictions_final  \n",
       "0                   2                         2                  1  \n",
       "1                   3                         1                  2  \n",
       "2                   9                         1                  2  \n",
       "3                   5                         1                  0  \n",
       "4                   4                         2                  1  \n",
       "..                ...                       ...                ...  \n",
       "83                  6                         3                  2  \n",
       "84                  4                         1                  2  \n",
       "85                  2                         2                  1  \n",
       "86                  8                         3                  2  \n",
       "87                  5                         1                  0  \n",
       "\n",
       "[88 rows x 6 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "7ef66e19-124d-40c4-acb5-26607fa5f4d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    49\n",
       "1    27\n",
       "0    12\n",
       "Name: predictions_final, dtype: int64"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df.predictions_final.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "901f4dfa-6de5-47cf-92ad-e6ac8a5b857f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1] [1 0]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#freq = np.bincount(predictions_df.iloc[87][('predictions', 'list')])\n",
    "l = [1]\n",
    "freq = np.bincount(l)\n",
    "freq_index = np.flip(np.argsort(freq))\n",
    "\n",
    "#l[winners[0]]\n",
    "if len(freq) == 1:\n",
    "    print(freq_index[0])\n",
    "if freq[freq_index[0]] == freq[freq_index[1]]:\n",
    "    print(1)\n",
    "elif freq[freq_index[0]] > freq[freq_index[1]]:\n",
    "    print(freq_index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "06197515-492f-4262-816f-56cd57d96cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# take the one with largest frequency.\n",
    "# when there is a draw (c0=c1=v2) (c0=c2) (c1=c2), (c0=c1) return vague.\n",
    "def decision(row):\n",
    "    freq = np.bincount(row[('predictions', 'list')])\n",
    "    freq_index = np.flip(np.argsort(freq))\n",
    "    if len(freq) == 1:\n",
    "        return freq_index[0]\n",
    "    elif freq[freq_index[0]] == freq[freq_index[1]]:\n",
    "        return 1 # vague\n",
    "    elif freq[freq_index[0]] > freq[freq_index[1]]:\n",
    "        return freq_index[0]\n",
    "\n",
    "\n",
    "        \n",
    "predictions_df['predictions_final'] = predictions_df.apply(decision, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9eb7e2-f902-415a-b293-b7f3ef763636",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Example from \n",
    "https://huggingface.co/docs/transformers/tasks/language_modeling#preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd03f71c-d8a5-49f6-8707-6aa0330f5c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilroberta-base\")\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer([\" \".join(x) for x in examples[\"answers.text\"]])\n",
    "\n",
    "block_size = 128\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    print(concatenated_examples.keys())\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of block_size.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23643d32-0381-41ef-a184-02e67d82586f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset eli5 (/home/jupyter/.cache/huggingface/datasets/eli5/LFQA_reddit/1.0.0/17574e5502a10f41bbd17beba83e22475b499fa62caa1384a3d093fc856fe6fa)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (693 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1311 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (651 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (982 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1483 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (801 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1161 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "eli5 = load_dataset(\"eli5\", split=\"train_asks[:5000]\")\n",
    "eli5 = eli5.flatten()\n",
    "eli5 = eli5.train_test_split(test_size=0.2)\n",
    "tokenized_eli5 = eli5.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=eli5[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c58dc9-0c98-4092-9d1d-92082616b932",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': ['input_ids', 'attention_mask'],\n",
       " 'test': ['input_ids', 'attention_mask']}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_eli5.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb844a4-ea4d-43b8-b27e-517373694dcb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"dict\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#tokenized_eli5.keys()\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtokenized_ds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"dict\") to list"
     ]
    }
   ],
   "source": [
    "#tokenized_eli5.keys()\n",
    "sum(tokenized_ds['train'], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c36e89-4021-4c1f-afaa-67b203494c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddb071f-d7b4-41d2-964b-3918b6f52c19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm_dataset['train'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac37050-1a30-448f-bc22-0e04cd01e1a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 4000\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_eli5['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49c5bb0-e03a-4242-9809-29ef4753bc92",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 19:41:22.795413: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-07 19:41:25.997772: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-06-07 19:41:25.997902: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-06-07 19:41:25.997913: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.data.data_collator because of the following error (look up to see its traceback):\n/opt/conda/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so: undefined symbol: cudaGraphInstantiateWithFlags, version libcudart.so.11.0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/import_utils.py:1172\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:992\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/data/__init__.py:27\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m glue_compute_metrics, xnli_compute_metrics\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocessors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     28\u001b[0m     DataProcessor,\n\u001b[1;32m     29\u001b[0m     InputExample,\n\u001b[1;32m     30\u001b[0m     InputFeatures,\n\u001b[1;32m     31\u001b[0m     SingleSentenceClassificationProcessor,\n\u001b[1;32m     32\u001b[0m     SquadExample,\n\u001b[1;32m     33\u001b[0m     SquadFeatures,\n\u001b[1;32m     34\u001b[0m     SquadV1Processor,\n\u001b[1;32m     35\u001b[0m     SquadV2Processor,\n\u001b[1;32m     36\u001b[0m     glue_convert_examples_to_features,\n\u001b[1;32m     37\u001b[0m     glue_output_modes,\n\u001b[1;32m     38\u001b[0m     glue_processors,\n\u001b[1;32m     39\u001b[0m     glue_tasks_num_labels,\n\u001b[1;32m     40\u001b[0m     squad_convert_examples_to_features,\n\u001b[1;32m     41\u001b[0m     xnli_output_modes,\n\u001b[1;32m     42\u001b[0m     xnli_processors,\n\u001b[1;32m     43\u001b[0m     xnli_tasks_num_labels,\n\u001b[1;32m     44\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/data/processors/__init__.py:16\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mglue\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m glue_convert_examples_to_features, glue_output_modes, glue_processors, glue_tasks_num_labels\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msquad\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SquadExample, SquadFeatures, SquadV1Processor, SquadV2Processor, squad_convert_examples_to_features\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataProcessor, InputExample, InputFeatures, SingleSentenceClassificationProcessor\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/data/processors/squad.py:34\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TensorDataset\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/__init__.py:229\u001b[0m\n\u001b[1;32m    228\u001b[0m         _load_global_deps()\n\u001b[0;32m--> 229\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# Appease the type checker; ordinarily this binding is inserted by the\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# torch._C module initialization code in C\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: /opt/conda/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so: undefined symbol: cudaGraphInstantiateWithFlags, version libcudart.so.11.0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataCollatorForLanguageModeling\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, TrainingArguments, Trainer\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/import_utils.py:1162\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1160\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1162\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1163\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/import_utils.py:1174\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1175\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1176\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1177\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.data.data_collator because of the following error (look up to see its traceback):\n/opt/conda/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so: undefined symbol: cudaGraphInstantiateWithFlags, version libcudart.so.11.0"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_eli5_clm-model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset[\"train\"],\n",
    "    eval_dataset=lm_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c01bf6e-451e-4c34-9fd5-1086e3e974eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m108"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
