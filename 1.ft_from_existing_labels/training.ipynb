{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e30e1cc-e117-4226-91a3-9fc36e8e3d1f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: transformers[torch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu116\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.13.1+cu116 (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.13.1+cu116\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# must run on activated terminal\n",
    "# i.e source activate py310\n",
    "!pip install --user transformers[torch] datasets evaluate\n",
    "!pip install --user torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae967541-2666-4a76-b04a-4fea870329dd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting wandb\n",
      "  Downloading wandb-0.15.4-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.31)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.28.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-1.25.1-py2.py3-none-any.whl (206 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m206.7/206.7 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (5.4.1)\n",
      "Collecting pathtools (from wandb)\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (67.7.2)\n",
      "Collecting appdirs>=1.4.3 (from wandb)\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.19.6)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n",
      "Building wheels for collected packages: pathtools\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=391a41948e2cd4f4c631053d7d55150c7690e30583bdc544f47cfce3bbc7abee\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
      "Successfully built pathtools\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ill (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: pathtools, appdirs, setproctitle, sentry-sdk, docker-pycreds, wandb\n",
      "Successfully installed appdirs-1.4.4 docker-pycreds-0.4.0 pathtools-0.1.2 sentry-sdk-1.25.1 setproctitle-1.3.2 wandb-0.15.4\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb579470-b7ee-41b2-95b9-9088bd8a8b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_caption</th>\n",
       "      <th>username</th>\n",
       "      <th>Openness</th>\n",
       "      <th>Conscientiousness</th>\n",
       "      <th>Extroversion</th>\n",
       "      <th>Agreeableness</th>\n",
       "      <th>Neuroticism</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>caption did you know henry ford made the 8 hou...</td>\n",
       "      <td>36mktg</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>caption so clean smiling_face_with_heart-eyesr...</td>\n",
       "      <td>6staroileaters</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>caption through the night crescent_moon ‚Ä¢ maet...</td>\n",
       "      <td>740project</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>caption take your social media accounts out of...</td>\n",
       "      <td>_boostfruit.com_</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>caption appreciate you excellencealways folded...</td>\n",
       "      <td>_cloutfinder</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>caption none, is of type image, has 1 comments...</td>\n",
       "      <td>wipmarketing</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>caption enjoy this video while i disappear for...</td>\n",
       "      <td>xcarmeeen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5461</th>\n",
       "      <td>caption power back.#fanggang | xfl, is of type...</td>\n",
       "      <td>xflvipers</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>caption summeeeeer! all the hard work of clean...</td>\n",
       "      <td>yogabody_uk</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>caption now booking grammy award winning artis...</td>\n",
       "      <td>zgroupla</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>266 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           full_caption          username  \\\n",
       "59    caption did you know henry ford made the 8 hou...            36mktg   \n",
       "69    caption so clean smiling_face_with_heart-eyesr...    6staroileaters   \n",
       "26    caption through the night crescent_moon ‚Ä¢ maet...        740project   \n",
       "57    caption take your social media accounts out of...  _boostfruit.com_   \n",
       "10    caption appreciate you excellencealways folded...      _cloutfinder   \n",
       "...                                                 ...               ...   \n",
       "71    caption none, is of type image, has 1 comments...      wipmarketing   \n",
       "14    caption enjoy this video while i disappear for...         xcarmeeen   \n",
       "5461  caption power back.#fanggang | xfl, is of type...         xflvipers   \n",
       "67    caption summeeeeer! all the hard work of clean...       yogabody_uk   \n",
       "84    caption now booking grammy award winning artis...          zgroupla   \n",
       "\n",
       "      Openness  Conscientiousness  Extroversion  Agreeableness  Neuroticism  \n",
       "59           2                  2             2              2            0  \n",
       "69           0                  2             1              1            1  \n",
       "26           2                  0             0              0            1  \n",
       "57           2                  0             2              1            1  \n",
       "10           2                  0             2              1            1  \n",
       "...        ...                ...           ...            ...          ...  \n",
       "71           2                  2             2              2            0  \n",
       "14           0                  0             2              0            2  \n",
       "5461         0                  2             2              2            0  \n",
       "67           2                  2             2              2            0  \n",
       "84           2                  0             2              1            1  \n",
       "\n",
       "[266 rows x 7 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b5deed-97a6-4375-8fcf-93727618aa50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69      caption so clean smiling_face_with_heart-eyesr...\n",
       "5490    caption best day of my life ü´∂heart_suitÔ∏èjetzt ...\n",
       "22      caption memorial day weekenTongue_sticking_out...\n",
       "5484    caption mission accomplished. mold broken. i l...\n",
       "47      caption anyone got a pre-order on one of these...\n",
       "                              ...                        \n",
       "108     caption logo variations thinking_face.....#dig...\n",
       "24      caption wah taj!!#taj 2nd season#congratulatio...\n",
       "136     caption it‚Äôs in our dna.big love,tbx, is of ty...\n",
       "14      caption enjoy this video while i disappear for...\n",
       "5461    caption power back.#fanggang | xfl, is of type...\n",
       "Name: full_caption, Length: 66, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df[feature_df['Openness']==0]['full_caption']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c00ef2-16df-444c-8b7f-5df771e933af",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.to_parquet(\"feature.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137de9b4-d099-488a-876f-4211389a58fb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'caption best day of my life \\U0001faf6heart_suitÔ∏èjetzt √ºberall erh√§ltlich firehundred_pointsfolded_hands‚Ä¢clip by: filmschieber_agency > _luck3 track by: komacasper x lucawolf_ #komacasper #vocaltekkno #vocaltekknoworldwide #lucawolf #ssb #festival #rave, is of type video, has 37 comments and 1804.0 likescaption sputnik springbreak 2023 es war absolut grandios smiling_face_with_heart-eyeshundred_pointsdankesch√∂n f√ºr dieses unvergessliche erlebnis, ihr habt es zu dem gemacht was es war 6000 menschen die bock auf vocaltekkno hatten! jeder der dabei war darf sich pers√∂nlich von mir gedr√ºckt f√ºhlen heart_suitÔ∏èfolded_handsdankesch√∂n bis 2024 double_exclamation_markÔ∏èsmiling_face_with_halohundred_points‚Ä¢video by: filmschieber_agency > _luck3 #komacasper #vocaltekkno #vocaltekknoworldwide #sputnik #festival #ssb #julianluckeproductions #rave, is of type video, has 65 comments and 3522.0 likescaption der sommer ist da und hier sind meine dates f√ºr juni und juli 2023 fire\\U0001faf6wir sehen uns hundred_pointsfolded_hands#komacasper #vocaltekkno #vocaltekknoworldwide #dates #sommer #openair #festivals, is of type image, has 35 comments and 1577.0 likescaption so ihr lieben ich melde mich zu wort firefolded_handswas war da bitte beim sputnik springbreak los \\U0001f979flushed_facees hat mich so ber√ºhrt das, dass zelt pl√∂tzlich so brechend voll war und wir nach 5 min einen einlass stopp hatten. face_screaming_in_fearred_heartÔ∏è ich danke euch f√ºr euren support und f√ºr diese unfassbare kranke stimmung. sign_of_the_hornssmiling_face_with_halored_heartÔ∏è‚Ä¢clip by: filmschieber_agency > _luck3 #komacasper #vocaltekkno #vocaltekknoworldwide #sputnik #rave #b√ºhne #julianluckeproductions, is of type video, has 168 comments and 4252.0 likescaption graf karl kassel es war fett hundred_pointsfirewir sehen uns wieder \\U0001faf6red_heartÔ∏è‚Ä¢movie by: filmschieber_agency > jstn_mhs #komacasper #vocaltekkno #vocaltekknoworldwide #rave #club #kassel, is of type video, has 29 comments and 3208.0 likescaption am freitag den 26.05.2023 kommt meine neue single best day of my life  zusammen mit luca wolf auf allen bekannten plattformen \\U0001faf6heart_suitÔ∏èfireverbreitet das ding ü•∞lucawolf_ #komacasper #vocaltekkno #vocaltekknoworldwide #new #single #lucawolf, is of type video, has 151 comments and 3226.0 likescaption jaa ich freu mich aufs album fire\\U0001faf6jetzt vorbestellen (siehe bioConfusion‚Ä¢camera_with_flash jstn_mhs #komacasper #vocaltekkno #vocaltekknoworldwide #album, is of type image, has 17 comments and 2843.0 likescaption der schutz in die ohren und auf geht es zu 5 auftritten folded_hands\\U0001faf6ready for this smiling_face_with_sunglassesfire‚Ä¢camera_with_flash zandergrafie #komacasper #vocaltekkno #vocaltekknoworldwide #rave #gig, is of type image, has 21 comments and 3229.0 likescaption wilde zicke egeln was war das denn f√ºr ein saunagang ü•µ geisteskranker abriss gepaart mit 95 aufguss face_with_stuck-out_tongue_&_winking_eyeface_with_open_mouth\\u200ddashing_away\\U0001fae0bis zum n√§chsten mal heart_suitÔ∏èfirehandshake‚Ä¢video by: _luck3 filmschieber_agency #komacasper #vocaltekkno #vocaltekknoworldwide #wildezicke #club #rave #julianluckeproductions #_luck3, is of type video, has 82 comments and 4825.0 likescaption raven k√∂nnen wir alter was f√ºr ein wochenende face_screaming_in_fearfirehundred_pointskrank hundred_pointssmiling_face_with_sunglassescamera_with_flash zandergrafie #komacasper #vocaltekkno #vocaltekknoworldwide #club, is of type image, has 29 comments and 3014.0 likescaption no one else jetzt √ºberall erh√§ltlich hundred_pointssmiling_face_with_halolink in biodouble_exclamation_markÔ∏è#komacasper #vocaltekkno #vocaltekknoworldwide, is of type video, has 81 comments and 3188.0 likescaption vocaltekkno in fuerteventura \\U0001faf6red_heartÔ∏è#komacasper #vocaltekkno #vocaltekknoworldwide #urlaub #fuerteventura #insel, is of type image, has 16 comments and 2937.0 likescaption tja jetzt steht eine 3 vor der 0 ü•≥face_with_tears_of_joyegal das ermutigt mich nur noch mehr abzurei√üen und vollgas zugeben. smiling_face_with_sunglasseshundred_pointsfiredanke an alle die zu meinem 20 hust 30 geburtstag dabei waren heart_suitÔ∏è\\U0001faf6und jetzt seht selbst das fette aftermovie von _luck3 sign_of_the_hornsfolded_handssmiling_face_with_halo‚Ä¢movie by: filmschieber_agency _luck3 #komacasper #vocaltekkno #vocaltekknoworldwide #remix #rave #geburtstag #_luck3 #julianluckeproductions, is of type video, has 67 comments and 3906.0 likescaption double_exclamation_markÔ∏èank√ºndigung double_exclamation_markÔ∏èes ist soweit ich darf es euch endlich verraten face_screaming_in_fearein riesen schritt in meiner laufbahn f√ºr mich firefolded_handsbestellt das ding vor smiling_face_with_sunglassesfolded_hands\\U0001faf6 link in bio smiling_face_with_halo#komacasper #vocaltekkno #vocaltekknoworldwide #album, is of type video, has 215 comments and 5947.0 likescaption k√∂ln wir haben den affen aus dem k√§fig gelassen face_with_stuck-out_tongue_&_winking_eyefolded_handsfette nacht bei euch  danke smiling_face_with_sunglassesheart_suitÔ∏èsign_of_the_horns‚Ä¢video by: _luck3 affenkaefig.events #komacasper #vocaltekkno #rave #affenk√§fig #k√∂ln #_luck3 #julianluckeproductions, is of type video, has 51 comments and 4120.0 likescaption meine mai dates 2023 ü§©sign_of_the_hornses wird sportlich und es geht endlich wieder nach drau√üen \\U0001faf6smiling_face_with_halowo sehen wir uns?‚Ä¢#komacasper #vocaltekkno #vocaltekknoworldwide #festival #rave #viral #openair, is of type image, has 44 comments and 1393.0 likescaption boha was ein abend in den sandsteinh√∂hlen halberstadt firehundred_pointsseht selber smiling_face_with_sunglassesfolded_handssmiling_face_with_halo‚Ä¢video by: _luck3 #komacasper #vocaltekkno #vocaltekknoworldwide #_luck3 #club #irre, is of type video, has 116 comments and 5967.0 likescaption mein buddy alfred_heinrichs und ich haben gestern mal kurz die h√∂hle zum beben gebracht und haben zusammen mein remix von alf seiner single irre performt face_with_stuck-out_tongue_&_winking_eyekranke vibes vereint in iammelodictechno und vocaltekkno right-facing_fistleft-facing_fist\\U0001faf6smiling_face_with_halo‚Ä¢clip by: _luck3 #komacasper #vocaltekkno #vocaltekknoworldwide #alfredheinrichs #iammelodictechno #_luck3 #club #performance, is of type video, has 172 comments and 6140.0 likescaption emotionen und energie verspr√ºhen, dass kann ich am wochenende am besten \\U0001faf6smiling_face_with_halo#komacasper #vocaltekkno #vocaltekknoworldwide #club, is of type image, has 57 comments and 5253.0 likescaption genau mein ding smiling_face_with_sunglasses sch√∂nes bergfest fire‚Ä¢camera_with_flash jstn_mhs #komacasper #vocaltekkno #vocaltekknoworldwide #foto, is of type image, has 16 comments and 2733.0 likescaption hamburg meine perlesmiling_face_with_heart-eyes das war ja fett im docks hundred_points eine grandiose nacht sign_of_the_hornsfire‚Ä¢p.s. der track kommt am freitag den 07.04.2023 auf allen plattformen.alfred heinrichs feat. haexxa - irre (komacasper remixConfusionalfred_heinrichs supdub_records ‚Ä¢movie by: _luck3 #komacasper #vocaltekkno #vocaltekknoworldwide #alfredheinrichs #dockshamburg #hamburg #remix #club #_luck3, is of type video, has 163 comments and 4961.0 likescaption welche energie in hamburg gestern im docks ü•µsmiling_face_with_halo‚Ä¢camera_with_flash _luck3 #komacasper #vocaltekkno #vocaltekknoworldwide #luck3, is of type image, has 23 comments and 1906.0 likescaption meine april dates 2023sign_of_the_hornssmiling_face_with_halowo sehen und h√∂ren wir uns?see you later smiling_face_with_halosmiling_face_with_sunglasses\\U0001faf6‚Ä¢#dates #komacasper #vocaltekkno #vocaltekknoworldwide, is of type image, has 68 comments and 1396.0 likescaption bass,bass ich habe bass see-no-evil_monkeysmiling_face_with_sunglasses\\U0001faf6auf geht es ins wochenende hundred_points‚Ä¢fr: ostarena lauchhammersa: labyrinth w√ºrzburgsign_of_the_hornscamera_with_flash _luck3 #komacasper #vocaltekkno #vocaltekknoworldwide #julianluckeproductions #rave #auftritt #show #club, is of type image, has 24 comments and 2237.0 likescaption easy bang bang smiling_face_with_sunglassescentral club erfurt das war irre ü•µeine glatte 10 von 10 ü§©smiling_face_with_halobis zum n√§chsten mal hundred_pointssign_of_the_horns‚Ä¢video by: _luck3 #komacasper #vocaltekkno #vocaltekknoworldwide #julianluckeproductions #_luck3 #erfurt #club #rave, is of type video, has 54 comments and 4093.0 likescaption das wochenende bleibt in erinnerung!ü•µ3 dicke shows und dann der kr√∂nende abschluss in erfurt im central club smiling_face_with_halofiredanke f√ºr diese energie red_heartÔ∏è\\U0001faf6‚Ä¢camera_with_flash by _luck3 #komacasper #vocaltekkno #vocaltekknoworldwide #club #show #erfurt #wochenende, is of type image, has 12 comments and 1788.0 likescaption dieses jahr wird es vocaltekkno baby face_with_stuck-out_tongue_&_winking_eyehundred_pointsfirecamera_with_flash jstn_mhs #komacasper #vocaltekkno #vocaltekknoworldwide, is of type image, has 35 comments and 3879.0 likescaption diese energie und kreativit√§t die durch meine adern flie√üt ist enorm ü§© dieses jahr werdet ihr davon noch viel zu sp√ºren bekommen \\U0001fae1 versprochensmiling_face_with_smiling_eyes‚Ä¢camera_with_flash marvin_schoenfelder #komacasper #vocaltekkno #vocaltekknoworldwide #club #energie #rave #viral, is of type image, has 15 comments and 2218.0 likescaption seit langem mal wieder im e werk reichenbach, was eine stimmung und ein genialer abriss \\U0001faf6\\U0001fae1bis zum n√§chsten mal fireperson_raising_hand\\u200dmale_signÔ∏èü•∞‚Ä¢movie by: _luck3 #komacasper #vocaltekkno #vocaltekknoworldwide #_luck3 #julianluckeproductions #club #remix, is of type video, has 65 comments and 3967.0 likescaption auf geht es ins wochenende grinning_face_with_smiling_eyescamera_with_flash _luck3 #komacasper #vocaltekkno #vocaltekknoworldwide #julianluckeproductions #rave, is of type image, has 13 comments and 2603.0 likescaption dates m√§rz 2023 hundred_pointssign_of_the_horns#komacasper #vocaltekkno #vocaltekknoworldwide #dates, is of type image, has 15 comments and 1068.0 likescaption kick that bass ü§©firehundred_pointscamera_with_flash marvin_schoenfelder #komacasper #vocaltekkno #vocaltekknoworldwide #club, is of type image, has 9 comments and 2116.0 likescaption neues lied, geiler club, geile crowd ü•µsweat_dropletsich hatte die ehre,,better days‚Äò‚Äò in zusammenarbeit mit blvckcrowz in der factory magdeburg zu spielen und die stimmung war einfach krank, aber seht selber \\U0001faf6fireihr k√∂nnt den track jetzt √ºberall streamen smiling_face_with_sunglassesdanke an craigmortalis f√ºr die einladung handshakered_heartÔ∏è‚Ä¢riesen respekt an _luck3 f√ºr diese krassen momentaufnahme einer vollkommensten eskalation smiling_face_with_heart-eyes\\U0001fae1\\U0001faf6red_heartÔ∏è#komacasper #blvckcrowz #vocaltekkno #vocaltekknoworldwide #club #single #new, is of type video, has 33 comments and 2440.0 likescaption alwo altenburg war wild, habe geh√∂rt wir sehen uns im april wieder sign_of_the_hornshundred_pointsfire‚Ä¢movie by: _luck3 #komacasper #vocaltekkno #vocaltekknoworldwide #alwoaltenburg #julianluckeproductions, is of type video, has 17 comments and 2541.0 likescaption rave & bass & vocaltekkno smiling_face_with_sunglassessign_of_the_hornsfire‚Ä¢camera_with_flash _luck3 #komacasper #vocaltekkno #vocaltekknoworldwide #julianluckeproductions #club #rave, is of type image, has 9 comments and 2040.0 likescaption heute 00:00 uhr sign_of_the_hornsü•∞hundred_pointspresave link in story sign_of_the_hornsfire#komacasper #vocaltekkno #vocaltekknoworldwide #blvckcrowz #collab, is of type video, has 2 comments and 43.0 likescaption mauerpfeiffer saarbr√ºcken am freitag war einfach ne fette party smiling_face_with_heart-eyessign_of_the_hornshundred_pointstrack kommt am 24.02.2023 auf allen plattformen smiling_face_with_heart-eyessmiling_face_with_halohundred_pointsmovie by: _luck3 #komacasper #vocaltekkno #vocaltekknoworldwide #saarbr√ºcken #release #club #julianluckeproductions, is of type video, has 49 comments and 3197.0 likescaption diesen donnerstag hundred_pointsfireihr k√∂nnt das teil jetzt presaven \\U0001faf6\\U0001fae1(siehe storyConfusion#komacasper #blvckcrowz #collab, is of type video, has 18 comments and 363.0 likescaption was f√ºr ein grandioses wochenende 4 auftritte 1600 km, viel liebe und eine absolute eskalation ü•µ\\U0001faf6das herz ist f√ºr jeden der dabei war und es mit mir/uns genossen hat \\U0001faf6red_heartÔ∏èdankemauerpfeiffer ilovesnow.festival alwoclub strezzkidz_official stellwerk_halle ‚Ä¢einen sch√∂nen sonntag euch, auf das n√§chste wochenende \\U0001fae1firecamera_with_flash _luck3 #komacasper #vocaltekkno #vocaltekknoworldwide #club #deutschland #julianluckeproductions #ifeelthebass, is of type image, has 33 comments and 3286.0 likescaption mauerpfeiffer saarbr√ºcken war so dick, immer noch sprachlos √ºber die gestrige nacht red_heartÔ∏èfireü•µdankesch√∂n smiling_face_with_halored_heartÔ∏èclip by: _luck3 #komacasper #vocaltekkno #vocaltekknoworldwide #saarbr√ºcken #mauerpfeiffer #club #crowd #julianluckeproductions, is of type video, has 35 comments and 3231.0 likescaption i feel the bass fireü•µ‚Ä¢camera_with_flash _luck3 #komacasper #vocaltekkno #vocaltekknoworldwide #julianluckeproductions, is of type image, has 20 comments and 3013.0 likescaption helios k√∂ln es war mir eine ehre smiling_face_with_sunglasseshandshakesmiling_face_with_halofireich komme wieder das ist safe sign_of_the_hornsü§™‚Ä¢movie by: _luck3 #komacasper #vocaltekkno #vocaltekknoworldwide #k√∂ln #club #luck_3 #julianluckeproductions #unreal, is of type video, has 30 comments and 3027.0 likescaption dankesch√∂n helios k√∂ln sign_of_the_hornshundred_pointsdas war stabil und ich glaube wir haben die gebruederbrett_insta sportlich vertreten \\U0001fae1red_heartÔ∏ècamera_with_flash _luck3 #komacasper #vocaltekkno #vocaltekknoworldwide #julianluckeproductions #k√∂ln #club, is of type image, has 6 comments and 1566.0 likescaption meine februar dates 2023 \\U0001faf6fireich freu mich auf ein dicken monat face_with_stuck-out_tongue_&_closed_eyeshundred_points#komacasper #vocaltekkno #vocaltekknoworldwide #dates #february, is of type image, has 51 comments and 1254.0 likescaption ey was war da los im sax d√∂lzig ü•µ 1300 leute am eskalieren face_with_open_mouth_&_cold_sweatfett sign_of_the_hornsfiredanke f√ºr diese fette nacht smiling_face_with_haloü•∞\\U0001faf6‚Ä¢video by: marvin_schoenfelder #komacasper #vocaltekkno #vocaltekknoworldwide #club #remix #d√∂lzig, is of type video, has 54 comments and 3957.0 likescaption wow was f√ºr ein wochenende smiling_face_with_sunglassesfirehirsch n√ºrnberg und sax d√∂lzig danke f√ºr diesen stabilen tanz \\U0001fae1red_heartÔ∏è‚Ä¢camera_with_flash marvin_schoenfelder #komacasper #vocaltekkno #vocaltekknoworldwide #club #marvinschoenfelder, is of type image, has 11 comments and 2170.0 likescaption ich h√§nge immer noch im letzten wochenende fest ü§≠smiling_face_with_sunglasseswien war stark und ich bin sprachlos.üá¶üáπhandshakedie tage im studio sind auch ein genuss der kreativit√§t smiling_face_with_halo\\U0001faf6bald mehr dazu, sch√∂nes wochenende euch allen ü•∞camera_with_flash mikejonesfotografie #komacasper #vocaltekkno #wien #gl√ºcklich #affenk√§fig, is of type image, has 16 comments and 1761.0 likescaption alter √∂sterreich üá¶üáπ das war geisteskrank smiling_face_with_heart-eyesfiredanke das ihr mich so lieb empfangen habt in einem neuen land, es war eine wahnsinnig tolle erfahrung in einem anderen land zu spielen,daf√ºr bin ich sehr dankbar!red_heartÔ∏èdankesch√∂n wien und dankesch√∂n an die affenk√§fig crewred_heartÔ∏èp.s. der track kommt am 09.02.2023 es ist eine fette collab zusammen mit meinem bruder blvck crowz right-facing_fistleft-facing_fist‚Ä¢movie by: _luck3 track by: blvckcrowz _komacasper_ #komacasper #vocaltekkno #vocaltekknoworldwide #newmusic #viral #wien #ostereich #affenk√§fig #julianluckeproductions, is of type video, has 67 comments and 3262.0 likescaption du,du,du und du backhand_index_pointing_downthanks for the support hundred_pointssmiling_face_with_halored_heartÔ∏è#komacasper #vocaltekkno #vocaltekknoworldwide, is of type image, has 16 comments and 2117.0 likescaption 2023 ich bin readyfiresmiling_face_with_haloneue musik,neue festivals,neue clubs.ich bin hyped auf ein ein aufregendes jahr smiling_face_with_sunglassesfire#komacasper #vocaltekkno #vocaltekknoworldwide #2023 #club #festivals, is of type image, has 8 comments and 1819.0 likes'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth=100\n",
    "feature_df[feature_df['Openness']==0].full_caption.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f451131b-6f15-4482-a569-77e390dd61b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0    200\n",
       "0.0     66\n",
       "Name: Openness, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df['Openness'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b2c2af-dd17-4c13-9153-e32e947eef0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    61\n",
       "2.0     1\n",
       "Name: Conscientiousness, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df['Conscientiousness'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900b5c6b-848d-455d-9392-16fc202c058d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    229\n",
       "0     27\n",
       "1     10\n",
       "Name: Extroversion, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df['Extroversion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052af9c9-373b-411d-99cb-f418bf2f3de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    139\n",
       "0     82\n",
       "1     45\n",
       "Name: Agreeableness, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df['Agreeableness'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c077b0f-320b-4416-81e5-0c5b7ff7ec85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    194\n",
       "1     45\n",
       "2     27\n",
       "Name: Neuroticism, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df['Neuroticism'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26a7446d-19cf-4041-b742-c3ef6d5050f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/266 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13472 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4864 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3007 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25039 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/266 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 8977\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2245\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from transformers import AutoTokenizer\n",
    "# using disilroberta as a quick baseline model\n",
    "# https://huggingface.co/docs/transformers/tasks/sequence_classification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\", use_fast=True)\n",
    "\n",
    "focused_trait = \"Neuroticism\"\n",
    "\n",
    "\n",
    "def tokenize_input(examples):\n",
    "    return tokenizer(examples[\"full_caption\"])\n",
    "\n",
    "def group_texts(examples):\n",
    "    block_size = 128    \n",
    "    \n",
    "    result= defaultdict(list)\n",
    "    for i, input_id  in enumerate(examples['input_ids']):\n",
    "        attn_mask = examples['attention_mask'][i]\n",
    "        total_length = len(input_id)\n",
    "        if total_length > block_size:\n",
    "            total_length = (total_length // block_size) * block_size\n",
    "        num_chunks = total_length // block_size\n",
    "        result['input_ids'].extend([input_id[i : i + block_size] for i in range(0, total_length, block_size)])\n",
    "        result['attention_mask'].extend([attn_mask[i : i + block_size] for i in range(0, total_length, block_size)])\n",
    "        result['Openness'].extend([examples['Openness'][i] for _ in range(num_chunks)])\n",
    "        result['Conscientiousness'].extend([examples['Conscientiousness'][i] for _ in range(num_chunks)])\n",
    "        result['Extroversion'].extend([examples['Extroversion'][i] for _ in range(num_chunks)])\n",
    "        result['Agreeableness'].extend([examples['Agreeableness'][i] for _ in range(num_chunks)])\n",
    "        result['Neuroticism'].extend([examples['Neuroticism'][i] for _ in range(num_chunks)])    \n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "import pandas as pd \n",
    "all_five = ['Openness', 'Conscientiousness','Extroversion', 'Agreeableness', 'Neuroticism']\n",
    "feature_df = pd.read_parquet(\"feature.parquet\")\n",
    "feature_df = feature_df.dropna()\n",
    "\n",
    "d = {0.5:1,1.0:2,0:0}\n",
    "for column in all_five:\n",
    "    feature_df[column] = feature_df[column].map(d)\n",
    "    feature_df.astype({column:'int32'}).dtypes\n",
    "\n",
    "all_five.remove(focused_trait)\n",
    "\n",
    "ds = Dataset.from_dict(feature_df.to_dict('list'))\n",
    "tokenized_ds = ds.map(tokenize_input, batched=True, num_proc=4, remove_columns=['full_caption','username'])\n",
    "lm_dataset_ig = tokenized_ds.map(group_texts, num_proc=4, batched=True)\n",
    "lm_dataset_ig = lm_dataset_ig.shuffle(seed=42)\n",
    "lm_dataset_ig = lm_dataset_ig.rename_column(focused_trait, \"label\")\n",
    "lm_dataset_ig = lm_dataset_ig.remove_columns(all_five)\n",
    "lm_dataset_ig = lm_dataset_ig.train_test_split(test_size=0.2)\n",
    "lm_dataset_ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6939a417-9b6c-4e23-8066-1ac20e42b2a4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_dataset_ig['train']['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba6d5c4-ee1d-4467-b3f5-19aef89a8911",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dataset IO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef70b124-bc97-4e03-8346-3befd08a96a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/8977 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2245 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_dataset_ig.save_to_disk(\"distilbert_base_block128_train_test_split_tokenized_feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249a27ce-5775-4ca4-865e-ae93ba7f5913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "lm_dataset_ig = load_from_disk(\"distilbert_base_block128_train_test_split_tokenized_feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9638013-98af-44c3-94fc-8f2b3fcb7ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename Agreeableness to label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545caef6-9b6a-48de-bba4-9b259c7eadca",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " ...]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_dataset_ig['train']['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edb6dac6-a840-4fa4-8950-ff2a98033dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    print(f\"predictions:{predictions}, item_freq:{np.unique(predictions, return_counts=True)}.\")\n",
    "    print(f\"labels:{labels}, item_freq:{np.unique(labels, return_counts=True)}.\")\n",
    "    accuracy_measurement =  accuracy.compute(predictions=predictions, references=labels)\n",
    "    precision_measurement =  precision.compute(predictions=predictions, references=labels, average='micro')\n",
    "    recall_measurement =  recall.compute(predictions=predictions, references=labels, average='micro')\n",
    "    print(f\"Accuracy:{accuracy_measurement}, Precision:{precision_measurement}, Recall:{recall_measurement}\")\n",
    "    return precision_measurement\n",
    "\n",
    "id2label = {0: \"LOW\", 1: \"VAGUE\", 2: \"HIGH\"}\n",
    "label2id = {\"LOW\": 0, \"VAGUE\":1, \"HIGH\": 2}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=3, id2label=id2label, label2id=label2id\n",
    ")\n",
    "import os \n",
    "os.environ[\"WANDB_PROJECT\"]=\"upwork-ocean-classification\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"]=\"true\"\n",
    "os.environ[\"WANDB_WATCH\"]=\"false\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"distilbert-base-uncased-{focused_trait}-classification-shuffled\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"wandb\",\n",
    "    push_to_hub=False,\n",
    "    run_name=focused_trait\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset_ig[\"train\"],\n",
    "    eval_dataset=lm_dataset_ig[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78b49851-07e0-4d84-bb23-d8a9be446c21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/uw-ocean/wandb/run-20230620_001307-tj18nhki</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bohao-cao/upwork-ocean-classification/runs/tj18nhki' target=\"_blank\">Neuroticism</a></strong> to <a href='https://wandb.ai/bohao-cao/upwork-ocean-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bohao-cao/upwork-ocean-classification' target=\"_blank\">https://wandb.ai/bohao-cao/upwork-ocean-classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bohao-cao/upwork-ocean-classification/runs/tj18nhki' target=\"_blank\">https://wandb.ai/bohao-cao/upwork-ocean-classification/runs/tj18nhki</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2810' max='2810' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2810/2810 09:40, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.522900</td>\n",
       "      <td>0.325799</td>\n",
       "      <td>0.885969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.258100</td>\n",
       "      <td>0.304947</td>\n",
       "      <td>0.909577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.152200</td>\n",
       "      <td>0.251792</td>\n",
       "      <td>0.938085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.083800</td>\n",
       "      <td>0.260075</td>\n",
       "      <td>0.946548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.042900</td>\n",
       "      <td>0.254726</td>\n",
       "      <td>0.946993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions:[0 0 0 ... 1 0 0], item_freq:(array([0, 1, 2]), array([1845,  360,   40])).\n",
      "labels:[0 0 0 ... 1 0 0], item_freq:(array([0, 1, 2]), array([1733,  356,  156])).\n",
      "Accuracy:{'accuracy': 0.8859688195991091}, Precision:{'precision': 0.8859688195991091}, Recall:{'recall': 0.8859688195991091}\n",
      "predictions:[0 0 0 ... 1 0 0], item_freq:(array([0, 1, 2]), array([1816,  375,   54])).\n",
      "labels:[0 0 0 ... 1 0 0], item_freq:(array([0, 1, 2]), array([1733,  356,  156])).\n",
      "Accuracy:{'accuracy': 0.9095768374164811}, Precision:{'precision': 0.9095768374164811}, Recall:{'recall': 0.9095768374164811}\n",
      "predictions:[0 0 0 ... 1 0 0], item_freq:(array([0, 1, 2]), array([1826,  283,  136])).\n",
      "labels:[0 0 0 ... 1 0 0], item_freq:(array([0, 1, 2]), array([1733,  356,  156])).\n",
      "Accuracy:{'accuracy': 0.9380846325167038}, Precision:{'precision': 0.9380846325167038}, Recall:{'recall': 0.9380846325167038}\n",
      "predictions:[0 0 0 ... 1 0 0], item_freq:(array([0, 1, 2]), array([1779,  317,  149])).\n",
      "labels:[0 0 0 ... 1 0 0], item_freq:(array([0, 1, 2]), array([1733,  356,  156])).\n",
      "Accuracy:{'accuracy': 0.9465478841870824}, Precision:{'precision': 0.9465478841870824}, Recall:{'recall': 0.9465478841870824}\n",
      "predictions:[0 0 0 ... 1 0 0], item_freq:(array([0, 1, 2]), array([1778,  328,  139])).\n",
      "labels:[0 0 0 ... 1 0 0], item_freq:(array([0, 1, 2]), array([1733,  356,  156])).\n",
      "Accuracy:{'accuracy': 0.9469933184855234}, Precision:{'precision': 0.9469933184855234}, Recall:{'recall': 0.9469933184855234}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñà‚ñÜ‚ñÅ‚ñÇ‚ñÅ</td></tr><tr><td>eval/precision</td><td>‚ñÅ‚ñÑ‚ñá‚ñà‚ñà</td></tr><tr><td>eval/runtime</td><td>‚ñÖ‚ñà‚ñÑ‚ñÇ‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÑ‚ñÅ‚ñÖ‚ñá‚ñà</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÑ‚ñÅ‚ñÖ‚ñá‚ñà</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà</td></tr><tr><td>train/learning_rate</td><td>‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÅ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ</td></tr><tr><td>train/train_loss</td><td>‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.25473</td></tr><tr><td>eval/precision</td><td>0.94699</td></tr><tr><td>eval/runtime</td><td>8.6008</td></tr><tr><td>eval/samples_per_second</td><td>261.024</td></tr><tr><td>eval/steps_per_second</td><td>16.394</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>2810</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0429</td></tr><tr><td>train/total_flos</td><td>1486476305890560.0</td></tr><tr><td>train/train_loss</td><td>0.19167</td></tr><tr><td>train/train_runtime</td><td>585.4364</td></tr><tr><td>train/train_samples_per_second</td><td>76.669</td></tr><tr><td>train/train_steps_per_second</td><td>4.8</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Neuroticism</strong> at: <a href='https://wandb.ai/bohao-cao/upwork-ocean-classification/runs/tj18nhki' target=\"_blank\">https://wandb.ai/bohao-cao/upwork-ocean-classification/runs/tj18nhki</a><br/>Synced 6 W&B file(s), 0 media file(s), 7 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230620_001307-tj18nhki/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "trainer.train()\n",
    "wandb.finish()\n",
    "trainer.save_state()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fa1111-b862-4623-ba37-4bc37c962163",
   "metadata": {},
   "source": [
    "## inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9f1a82e1-03de-4a54-8c7e-d39775bb69a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "inference_dataset = pd.read_parquet(\"prediction_feature.parquet\")\n",
    "prediction_dataset = pd.read_parquet(\"concat_feature.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a92228a1-b15b-4f2b-a507-3425f7a2ae06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_caption</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ig_username</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>007offmason</th>\n",
       "      <td>caption out now on all platformsglobe_showing_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0g.ed1tz</th>\n",
       "      <td>caption none, is of type image, has 6 comments...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000taraneh</th>\n",
       "      <td>caption .¬´ŸÑÿ¨ Ÿà ŸÑÿ¨ ÿ®ÿßÿ≤€å¬ª ÿ®ÿß ÿµÿØÿß€å ÿ≤ŸÜÿØŸá €åÿßÿØ ŸæŸàÿ±ÿßŸÜ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000thingstodoindenver</th>\n",
       "      <td>caption opening day specials: here are some fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101_molly</th>\n",
       "      <td>caption yessirr let‚Äôs go!! firefireü©∏red_heartÔ∏è...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zuber_zooby</th>\n",
       "      <td>caption titanic movie explained in hindi face_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zuraidajardine</th>\n",
       "      <td>caption unhappy fathers day ü©∂today can hit dif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zuri_baby</th>\n",
       "      <td>caption never in a million years did i think i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zyramusicofficial</th>\n",
       "      <td>caption thank you leannealamira for working yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zzward</th>\n",
       "      <td>caption meet my stylist!!! haha! #independenta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14240 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             full_caption\n",
       "ig_username                                                              \n",
       "007offmason             caption out now on all platformsglobe_showing_...\n",
       "0g.ed1tz                caption none, is of type image, has 6 comments...\n",
       "1000taraneh             caption .¬´ŸÑÿ¨ Ÿà ŸÑÿ¨ ÿ®ÿßÿ≤€å¬ª ÿ®ÿß ÿµÿØÿß€å ÿ≤ŸÜÿØŸá €åÿßÿØ ŸæŸàÿ±ÿßŸÜ...\n",
       "1000thingstodoindenver  caption opening day specials: here are some fu...\n",
       "101_molly               caption yessirr let‚Äôs go!! firefireü©∏red_heartÔ∏è...\n",
       "...                                                                   ...\n",
       "zuber_zooby             caption titanic movie explained in hindi face_...\n",
       "zuraidajardine          caption unhappy fathers day ü©∂today can hit dif...\n",
       "zuri_baby               caption never in a million years did i think i...\n",
       "zyramusicofficial       caption thank you leannealamira for working yo...\n",
       "zzward                  caption meet my stylist!!! haha! #independenta...\n",
       "\n",
       "[14240 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "08443d39-88e4-4a9e-8b29-146ba29d1ebf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/14240 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from collections import defaultdict\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\", use_fast=True)\n",
    "\n",
    "def tokenize_input(examples):\n",
    "    return tokenizer(examples[\"full_caption\"], padding=True)\n",
    "\n",
    "def group_texts_for_inference(examples):\n",
    "    block_size = 512    \n",
    "    \n",
    "    result= defaultdict(list)\n",
    "    for i, input_id  in enumerate(examples['input_ids']):\n",
    "        attn_mask = examples['attention_mask'][i]\n",
    "        total_length = len(input_id)\n",
    "        if total_length < block_size:\n",
    "            result['input_ids'].extend([input_id])\n",
    "            result['attention_mask'].extend([attn_mask])\n",
    "            result['ig_username'].extend([examples['ig_username'][i]])\n",
    "            continue                        \n",
    "        if total_length > block_size:\n",
    "            total_length = (total_length // block_size) * block_size\n",
    "        num_chunks = total_length // block_size\n",
    "        with open (\"log.log\", \"a\") as f_out:\n",
    "            f_out.write(f\"{i} {total_length} {block_size} {num_chunks}\\n\")\n",
    "        result['input_ids'].extend([input_id[i : i + block_size] for i in range(0, total_length, block_size)])\n",
    "        result['attention_mask'].extend([attn_mask[i : i + block_size] for i in range(0, total_length, block_size)])\n",
    "        result['ig_username'].extend([examples['ig_username'][i] for _ in range(num_chunks)])\n",
    "    return result\n",
    "\n",
    "\n",
    "#ds = Dataset.from_dict(inference_dataset.reset_index().to_dict('list'))\n",
    "#tokenized_ds = ds.map(tokenize_input, batched=True, num_proc=4, remove_columns=['full_caption'])\n",
    "lm_dataset_ig = tokenized_ds.map(group_texts_for_inference, num_proc=4, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25368aa9-cf92-4d0f-937f-04cdba797060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lm_dataset_ig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2475e162-bd30-42eb-b739-3cee07de97b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "lm_dataset_ig = load_from_disk(\"grouped_prediction_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1998c77c-1ff9-49a8-afd6-374d59c3b739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/3 shards):   0%|          | 0/409880 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_dataset_ig.save_to_disk(\"grouped_prediction_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6ecf1e9-321f-4115-b5b2-100992cf1084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/14240 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_ds.save_to_disk(\"tokenized_prediction_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c92109f-312b-4c7c-958d-fea517127e8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DistilBertModel, DistilBertForSequenceClassification\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lm_dataset_ig_gpu = lm_dataset_ig.with_format(\"torch\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "761e125d-5975-47db-970a-acaaedc4a5d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-01 02:08:50.618016: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-01 02:08:53.296281: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-07-01 02:08:53.296427: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-07-01 02:08:53.296439: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "focused_trait = \"Agreeableness\"\n",
    "ft_model = AutoModelForSequenceClassification.from_pretrained(f\"distilbert-base-uncased-{focused_trait}-classification-shuffled\")\n",
    "ft_model = ft_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6475fc-df58-49a1-bab9-251b1884be77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "40\n",
      "80\n",
      "120\n",
      "160\n",
      "200\n",
      "240\n",
      "280\n",
      "320\n",
      "360\n",
      "400\n",
      "440\n",
      "480\n",
      "520\n",
      "560\n",
      "600\n",
      "640\n",
      "680\n",
      "720\n",
      "760\n",
      "800\n",
      "840\n",
      "880\n",
      "920\n",
      "960\n",
      "1000\n",
      "1040\n",
      "1080\n",
      "1120\n",
      "1160\n",
      "1200\n",
      "1240\n",
      "1280\n",
      "1320\n",
      "1360\n",
      "1400\n",
      "1440\n",
      "1480\n",
      "1520\n",
      "1560\n",
      "1600\n",
      "1640\n",
      "1680\n",
      "1720\n",
      "1760\n",
      "1800\n",
      "1840\n",
      "1880\n",
      "1920\n",
      "1960\n",
      "2000\n",
      "2040\n",
      "2080\n",
      "2120\n",
      "2160\n",
      "2200\n",
      "2240\n",
      "2280\n",
      "2320\n",
      "2360\n",
      "2400\n",
      "2440\n",
      "2480\n",
      "2520\n",
      "2560\n",
      "2600\n",
      "2640\n",
      "2680\n",
      "2720\n",
      "2760\n",
      "2800\n",
      "2840\n",
      "2880\n",
      "2920\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "all_predictions = pd.DataFrame()\n",
    "batch_size = 40\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(lm_dataset_ig_gpu), batch_size):\n",
    "        predictions = ft_model(\n",
    "            input_ids=lm_dataset_ig_gpu['input_ids'][i: i+batch_size],\n",
    "            attention_mask=lm_dataset_ig_gpu['attention_mask'][i: i+batch_size]\n",
    "        ).logits.argmax(axis=1)\n",
    "        predictions_df_current_batch = pd.DataFrame(\n",
    "            {\n",
    "                \"usernames\":lm_dataset_ig['ig_username'][i: i+batch_size], \n",
    "                \"predictions\":predictions.to('cpu')}\n",
    "            )\n",
    "        pd.concat([all_predictions, predictions_df_current_batch])\n",
    "        if i%10==0:\n",
    "            print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c1bb2b-9749-4c29-a448-9477dd96e75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Post process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "687fc22f-8cc8-4598-b5d4-3abe538817e2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">predictions</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>list</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>usernames</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2jpantoja</th>\n",
       "      <td>[2, 0]</td>\n",
       "      <td>{0, 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_moneybo</th>\n",
       "      <td>[2, 2, 2]</td>\n",
       "      <td>{2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_ts97__</th>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2]</td>\n",
       "      <td>{2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aless_capparelli</th>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>{0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alexsaenz</th>\n",
       "      <td>[2, 1, 1, 1]</td>\n",
       "      <td>{1, 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vngnc</th>\n",
       "      <td>[2, 2, 2, 1, 1, 0]</td>\n",
       "      <td>{0, 1, 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>waynebutlercomedy</th>\n",
       "      <td>[2, 2, 2, 2]</td>\n",
       "      <td>{2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>woolimusic</th>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>{0, 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zairmontes</th>\n",
       "      <td>[2, 1, 0, 2, 2, 2, 2, 2]</td>\n",
       "      <td>{0, 1, 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zambezijuice</th>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>{0}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   predictions           \n",
       "                                          list        set\n",
       "usernames                                                \n",
       "2jpantoja                               [2, 0]     {0, 2}\n",
       "_moneybo                             [2, 2, 2]        {2}\n",
       "_ts97__            [2, 2, 2, 2, 2, 2, 2, 2, 2]        {2}\n",
       "aless_capparelli               [0, 0, 0, 0, 0]        {0}\n",
       "alexsaenz                         [2, 1, 1, 1]     {1, 2}\n",
       "...                                        ...        ...\n",
       "vngnc                       [2, 2, 2, 1, 1, 0]  {0, 1, 2}\n",
       "waynebutlercomedy                 [2, 2, 2, 2]        {2}\n",
       "woolimusic                              [1, 0]     {0, 1}\n",
       "zairmontes            [2, 1, 0, 2, 2, 2, 2, 2]  {0, 1, 2}\n",
       "zambezijuice                   [0, 0, 0, 0, 0]        {0}\n",
       "\n",
       "[88 rows x 2 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df = predictions_df.groupby('usernames').agg([list, set])\n",
    "predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ff70ded9-2cb3-478d-9b65-3f64bd623cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df.columns = predictions_df.columns.to_flat_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "813a6255-fbf3-46f6-a752-394bc41c279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = predictions_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e6fa63f1-a2bf-4346-8191-416e93fc03e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>usernames</th>\n",
       "      <th>(predictions, list)</th>\n",
       "      <th>(predictions, set)</th>\n",
       "      <th>predictions_count</th>\n",
       "      <th>predictions_unique_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2jpantoja</td>\n",
       "      <td>[2, 0]</td>\n",
       "      <td>{0, 2}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>_moneybo</td>\n",
       "      <td>[2, 2, 2]</td>\n",
       "      <td>{2}</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_ts97__</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2]</td>\n",
       "      <td>{2}</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aless_capparelli</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>{0}</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alexsaenz</td>\n",
       "      <td>[2, 1, 1, 1]</td>\n",
       "      <td>{1, 2}</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>vngnc</td>\n",
       "      <td>[2, 2, 2, 1, 1, 0]</td>\n",
       "      <td>{0, 1, 2}</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>waynebutlercomedy</td>\n",
       "      <td>[2, 2, 2, 2]</td>\n",
       "      <td>{2}</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>woolimusic</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>{0, 1}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>zairmontes</td>\n",
       "      <td>[2, 1, 0, 2, 2, 2, 2, 2]</td>\n",
       "      <td>{0, 1, 2}</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>zambezijuice</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>{0}</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            usernames          (predictions, list) (predictions, set)  \\\n",
       "0           2jpantoja                       [2, 0]             {0, 2}   \n",
       "1            _moneybo                    [2, 2, 2]                {2}   \n",
       "2             _ts97__  [2, 2, 2, 2, 2, 2, 2, 2, 2]                {2}   \n",
       "3    aless_capparelli              [0, 0, 0, 0, 0]                {0}   \n",
       "4           alexsaenz                 [2, 1, 1, 1]             {1, 2}   \n",
       "..                ...                          ...                ...   \n",
       "83              vngnc           [2, 2, 2, 1, 1, 0]          {0, 1, 2}   \n",
       "84  waynebutlercomedy                 [2, 2, 2, 2]                {2}   \n",
       "85         woolimusic                       [1, 0]             {0, 1}   \n",
       "86         zairmontes     [2, 1, 0, 2, 2, 2, 2, 2]          {0, 1, 2}   \n",
       "87       zambezijuice              [0, 0, 0, 0, 0]                {0}   \n",
       "\n",
       "    predictions_count  predictions_unique_count  \n",
       "0                   2                         2  \n",
       "1                   3                         1  \n",
       "2                   9                         1  \n",
       "3                   5                         1  \n",
       "4                   4                         2  \n",
       "..                ...                       ...  \n",
       "83                  6                         3  \n",
       "84                  4                         1  \n",
       "85                  2                         2  \n",
       "86                  8                         3  \n",
       "87                  5                         1  \n",
       "\n",
       "[88 rows x 5 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df['predictions_count'] = predictions_df.apply(lambda r: len(r[('predictions','list')]), axis=1)\n",
    "predictions_df['predictions_unique_count'] = predictions_df.apply(lambda r: len(r[('predictions','set')]), axis=1)\n",
    "predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f9841740-eb4d-4a1d-9164-f740b02df10e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>usernames</th>\n",
       "      <th>(predictions, list)</th>\n",
       "      <th>(predictions, set)</th>\n",
       "      <th>predictions_count</th>\n",
       "      <th>predictions_unique_count</th>\n",
       "      <th>predictions_final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2jpantoja</td>\n",
       "      <td>[2, 0]</td>\n",
       "      <td>{0, 2}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>_moneybo</td>\n",
       "      <td>[2, 2, 2]</td>\n",
       "      <td>{2}</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_ts97__</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2]</td>\n",
       "      <td>{2}</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aless_capparelli</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>{0}</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alexsaenz</td>\n",
       "      <td>[2, 1, 1, 1]</td>\n",
       "      <td>{1, 2}</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>vngnc</td>\n",
       "      <td>[2, 2, 2, 1, 1, 0]</td>\n",
       "      <td>{0, 1, 2}</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>waynebutlercomedy</td>\n",
       "      <td>[2, 2, 2, 2]</td>\n",
       "      <td>{2}</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>woolimusic</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>{0, 1}</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>zairmontes</td>\n",
       "      <td>[2, 1, 0, 2, 2, 2, 2, 2]</td>\n",
       "      <td>{0, 1, 2}</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>zambezijuice</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>{0}</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            usernames          (predictions, list) (predictions, set)  \\\n",
       "0           2jpantoja                       [2, 0]             {0, 2}   \n",
       "1            _moneybo                    [2, 2, 2]                {2}   \n",
       "2             _ts97__  [2, 2, 2, 2, 2, 2, 2, 2, 2]                {2}   \n",
       "3    aless_capparelli              [0, 0, 0, 0, 0]                {0}   \n",
       "4           alexsaenz                 [2, 1, 1, 1]             {1, 2}   \n",
       "..                ...                          ...                ...   \n",
       "83              vngnc           [2, 2, 2, 1, 1, 0]          {0, 1, 2}   \n",
       "84  waynebutlercomedy                 [2, 2, 2, 2]                {2}   \n",
       "85         woolimusic                       [1, 0]             {0, 1}   \n",
       "86         zairmontes     [2, 1, 0, 2, 2, 2, 2, 2]          {0, 1, 2}   \n",
       "87       zambezijuice              [0, 0, 0, 0, 0]                {0}   \n",
       "\n",
       "    predictions_count  predictions_unique_count  predictions_final  \n",
       "0                   2                         2                  1  \n",
       "1                   3                         1                  2  \n",
       "2                   9                         1                  2  \n",
       "3                   5                         1                  0  \n",
       "4                   4                         2                  1  \n",
       "..                ...                       ...                ...  \n",
       "83                  6                         3                  2  \n",
       "84                  4                         1                  2  \n",
       "85                  2                         2                  1  \n",
       "86                  8                         3                  2  \n",
       "87                  5                         1                  0  \n",
       "\n",
       "[88 rows x 6 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "7ef66e19-124d-40c4-acb5-26607fa5f4d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    49\n",
       "1    27\n",
       "0    12\n",
       "Name: predictions_final, dtype: int64"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df.predictions_final.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "901f4dfa-6de5-47cf-92ad-e6ac8a5b857f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1] [1 0]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#freq = np.bincount(predictions_df.iloc[87][('predictions', 'list')])\n",
    "l = [1]\n",
    "freq = np.bincount(l)\n",
    "freq_index = np.flip(np.argsort(freq))\n",
    "\n",
    "#l[winners[0]]\n",
    "if len(freq) == 1:\n",
    "    print(freq_index[0])\n",
    "if freq[freq_index[0]] == freq[freq_index[1]]:\n",
    "    print(1)\n",
    "elif freq[freq_index[0]] > freq[freq_index[1]]:\n",
    "    print(freq_index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "06197515-492f-4262-816f-56cd57d96cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# take the one with largest frequency.\n",
    "# when there is a draw (c0=c1=v2) (c0=c2) (c1=c2), (c0=c1) return vague.\n",
    "def decision(row):\n",
    "    freq = np.bincount(row[('predictions', 'list')])\n",
    "    freq_index = np.flip(np.argsort(freq))\n",
    "    if len(freq) == 1:\n",
    "        return freq_index[0]\n",
    "    elif freq[freq_index[0]] == freq[freq_index[1]]:\n",
    "        return 1 # vague\n",
    "    elif freq[freq_index[0]] > freq[freq_index[1]]:\n",
    "        return freq_index[0]\n",
    "\n",
    "\n",
    "        \n",
    "predictions_df['predictions_final'] = predictions_df.apply(decision, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9eb7e2-f902-415a-b293-b7f3ef763636",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Example from \n",
    "https://huggingface.co/docs/transformers/tasks/language_modeling#preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd03f71c-d8a5-49f6-8707-6aa0330f5c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilroberta-base\")\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer([\" \".join(x) for x in examples[\"answers.text\"]])\n",
    "\n",
    "block_size = 128\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    print(concatenated_examples.keys())\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of block_size.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23643d32-0381-41ef-a184-02e67d82586f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset eli5 (/home/jupyter/.cache/huggingface/datasets/eli5/LFQA_reddit/1.0.0/17574e5502a10f41bbd17beba83e22475b499fa62caa1384a3d093fc856fe6fa)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (693 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1311 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (651 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (982 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1483 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (801 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1161 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "eli5 = load_dataset(\"eli5\", split=\"train_asks[:5000]\")\n",
    "eli5 = eli5.flatten()\n",
    "eli5 = eli5.train_test_split(test_size=0.2)\n",
    "tokenized_eli5 = eli5.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=eli5[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c58dc9-0c98-4092-9d1d-92082616b932",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': ['input_ids', 'attention_mask'],\n",
       " 'test': ['input_ids', 'attention_mask']}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_eli5.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb844a4-ea4d-43b8-b27e-517373694dcb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"dict\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#tokenized_eli5.keys()\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtokenized_ds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"dict\") to list"
     ]
    }
   ],
   "source": [
    "#tokenized_eli5.keys()\n",
    "sum(tokenized_ds['train'], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c36e89-4021-4c1f-afaa-67b203494c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddb071f-d7b4-41d2-964b-3918b6f52c19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm_dataset['train'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac37050-1a30-448f-bc22-0e04cd01e1a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 4000\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_eli5['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49c5bb0-e03a-4242-9809-29ef4753bc92",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 19:41:22.795413: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-07 19:41:25.997772: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-06-07 19:41:25.997902: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-06-07 19:41:25.997913: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.data.data_collator because of the following error (look up to see its traceback):\n/opt/conda/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so: undefined symbol: cudaGraphInstantiateWithFlags, version libcudart.so.11.0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/import_utils.py:1172\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:992\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/data/__init__.py:27\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m glue_compute_metrics, xnli_compute_metrics\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocessors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     28\u001b[0m     DataProcessor,\n\u001b[1;32m     29\u001b[0m     InputExample,\n\u001b[1;32m     30\u001b[0m     InputFeatures,\n\u001b[1;32m     31\u001b[0m     SingleSentenceClassificationProcessor,\n\u001b[1;32m     32\u001b[0m     SquadExample,\n\u001b[1;32m     33\u001b[0m     SquadFeatures,\n\u001b[1;32m     34\u001b[0m     SquadV1Processor,\n\u001b[1;32m     35\u001b[0m     SquadV2Processor,\n\u001b[1;32m     36\u001b[0m     glue_convert_examples_to_features,\n\u001b[1;32m     37\u001b[0m     glue_output_modes,\n\u001b[1;32m     38\u001b[0m     glue_processors,\n\u001b[1;32m     39\u001b[0m     glue_tasks_num_labels,\n\u001b[1;32m     40\u001b[0m     squad_convert_examples_to_features,\n\u001b[1;32m     41\u001b[0m     xnli_output_modes,\n\u001b[1;32m     42\u001b[0m     xnli_processors,\n\u001b[1;32m     43\u001b[0m     xnli_tasks_num_labels,\n\u001b[1;32m     44\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/data/processors/__init__.py:16\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mglue\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m glue_convert_examples_to_features, glue_output_modes, glue_processors, glue_tasks_num_labels\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msquad\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SquadExample, SquadFeatures, SquadV1Processor, SquadV2Processor, squad_convert_examples_to_features\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataProcessor, InputExample, InputFeatures, SingleSentenceClassificationProcessor\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/data/processors/squad.py:34\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TensorDataset\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/__init__.py:229\u001b[0m\n\u001b[1;32m    228\u001b[0m         _load_global_deps()\n\u001b[0;32m--> 229\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# Appease the type checker; ordinarily this binding is inserted by the\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# torch._C module initialization code in C\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: /opt/conda/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so: undefined symbol: cudaGraphInstantiateWithFlags, version libcudart.so.11.0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataCollatorForLanguageModeling\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, TrainingArguments, Trainer\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/import_utils.py:1162\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1160\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1162\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1163\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/import_utils.py:1174\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1175\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1176\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1177\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.data.data_collator because of the following error (look up to see its traceback):\n/opt/conda/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so: undefined symbol: cudaGraphInstantiateWithFlags, version libcudart.so.11.0"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_eli5_clm-model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset[\"train\"],\n",
    "    eval_dataset=lm_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c01bf6e-451e-4c34-9fd5-1086e3e974eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m108"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
